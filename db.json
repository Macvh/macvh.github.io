{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/cactus/source/css/rtl.styl","path":"css/rtl.styl","modified":0,"renderable":1},{"_id":"themes/cactus/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/cactus/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/apple-touch-icon.png","path":"images/apple-touch-icon.png","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/favicon-192x192.png","path":"images/favicon-192x192.png","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/favicon.ico","path":"images/favicon.ico","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/logo.png","path":"images/logo.png","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/clipboard/clipboard.min.js","path":"lib/clipboard/clipboard.min.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/jquery/jquery.min.js","path":"lib/jquery/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Bold.ttf","path":"lib/meslo-LG/MesloLGL-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-BoldItalic.ttf","path":"lib/meslo-LG/MesloLGL-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Italic.ttf","path":"lib/meslo-LG/MesloLGL-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Regular.ttf","path":"lib/meslo-LG/MesloLGL-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Bold.ttf","path":"lib/meslo-LG/MesloLGM-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-BoldItalic.ttf","path":"lib/meslo-LG/MesloLGM-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Italic.ttf","path":"lib/meslo-LG/MesloLGM-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Regular.ttf","path":"lib/meslo-LG/MesloLGM-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Bold.ttf","path":"lib/meslo-LG/MesloLGS-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-BoldItalic.ttf","path":"lib/meslo-LG/MesloLGS-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Italic.ttf","path":"lib/meslo-LG/MesloLGS-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Regular.ttf","path":"lib/meslo-LG/MesloLGS-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.eot","path":"lib/vazir-font/Vazir-Black.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.ttf","path":"lib/vazir-font/Vazir-Black.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff","path":"lib/vazir-font/Vazir-Black.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff2","path":"lib/vazir-font/Vazir-Black.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.eot","path":"lib/vazir-font/Vazir-Bold.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.ttf","path":"lib/vazir-font/Vazir-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff","path":"lib/vazir-font/Vazir-Bold.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff2","path":"lib/vazir-font/Vazir-Bold.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.eot","path":"lib/vazir-font/Vazir-Light.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.ttf","path":"lib/vazir-font/Vazir-Light.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff","path":"lib/vazir-font/Vazir-Light.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff2","path":"lib/vazir-font/Vazir-Light.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.eot","path":"lib/vazir-font/Vazir-Medium.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.ttf","path":"lib/vazir-font/Vazir-Medium.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff","path":"lib/vazir-font/Vazir-Medium.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff2","path":"lib/vazir-font/Vazir-Medium.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.eot","path":"lib/vazir-font/Vazir-Regular.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.ttf","path":"lib/vazir-font/Vazir-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.woff","path":"lib/vazir-font/Vazir-Regular.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.woff2","path":"lib/vazir-font/Vazir-Regular.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.eot","path":"lib/vazir-font/Vazir-Thin.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.ttf","path":"lib/vazir-font/Vazir-Thin.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff","path":"lib/vazir-font/Vazir-Thin.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff2","path":"lib/vazir-font/Vazir-Thin.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.eot","path":"lib/vazir-font/Vazir-Variable.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.ttf","path":"lib/vazir-font/Vazir-Variable.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.woff","path":"lib/vazir-font/Vazir-Variable.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.woff2","path":"lib/vazir-font/Vazir-Variable.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/font-face.css","path":"lib/vazir-font/font-face.css","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.ttf","path":"lib/font-awesome/webfonts/fa-brands-400.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.ttf","path":"lib/font-awesome/webfonts/fa-regular-400.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-v4compatibility.woff2","path":"lib/font-awesome/webfonts/fa-v4compatibility.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.ttf","path":"lib/font-awesome/webfonts/fa-solid-900.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-v4compatibility.ttf","path":"lib/font-awesome/webfonts/fa-v4compatibility.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/justified-gallery/js/jquery.justifiedGallery.min.js","path":"lib/justified-gallery/js/jquery.justifiedGallery.min.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/justified-gallery/css/justifiedGallery.min.css","path":"lib/justified-gallery/css/justifiedGallery.min.css","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/mylogo.jpg","path":"images/mylogo.jpg","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/mylogo2.jpg","path":"images/mylogo2.jpg","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/myfavicon.ico","path":"images/myfavicon.ico","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1725192925301},{"_id":"source/about/index.md","hash":"525c51fe5af80d7bc5d045c80ea010780b48da7e","modified":1725193982752},{"_id":"themes/cactus/LICENSE","hash":"346ece39a983b0e7858c11f785cd846cef9eb875","modified":1725192993774},{"_id":"themes/cactus/.gitignore","hash":"72267ee409a324fc197c150b3c4bf28b87b709a8","modified":1725192993773},{"_id":"themes/cactus/.stylintrc","hash":"eb5f48e83657928cb0cbee031373b2cd36ca0083","modified":1725192993773},{"_id":"themes/cactus/gulpfile.js","hash":"70d419549ba72e0906fd2fc8103701142eb883a7","modified":1725192993774},{"_id":"themes/cactus/README.md","hash":"c497023057269cc3a6c66e5a6f207625041e9bd7","modified":1725192993774},{"_id":"themes/cactus/_config.yml","hash":"925b83cd22ec05d077a7830e3712c11c4518305b","modified":1725195991094},{"_id":"themes/cactus/package.json","hash":"dbb0a486006e7d4ecdb4b005b6a9b264b5d542dc","modified":1725192993778},{"_id":"themes/cactus/.jshintrc","hash":"2548bd6ce44422edc7e6f9f68061ab47f26c4f57","modified":1725192993773},{"_id":"themes/cactus/languages/ar.yml","hash":"81a88b0593fc89de3118d686681b1f69883c847b","modified":1725192993774},{"_id":"themes/cactus/languages/de.yml","hash":"43b2f4e078b042aaae0377a4235216a51ed82e0d","modified":1725192993774},{"_id":"themes/cactus/languages/default.yml","hash":"6a84970bf69c3e9490e5382747ca2b4c4b4dccde","modified":1725192993774},{"_id":"themes/cactus/languages/ca.yml","hash":"b79dd2c21dc6697c635e92db1f661a4b8d5d2305","modified":1725192993774},{"_id":"themes/cactus/languages/en.yml","hash":"6a84970bf69c3e9490e5382747ca2b4c4b4dccde","modified":1725192993774},{"_id":"themes/cactus/languages/es.yml","hash":"2b1fc8b0d636123e9ee39017fa20053bd1913a5a","modified":1725192993775},{"_id":"themes/cactus/languages/fa.yml","hash":"63f32e50953af1c4bd0308a4fca5862b5287c2cb","modified":1725192993775},{"_id":"themes/cactus/languages/it.yml","hash":"62800bcae1f2d2454f87f4bcf4d7593848424f61","modified":1725192993775},{"_id":"themes/cactus/languages/fr.yml","hash":"5c07406998f19d219a5a7b65c0d88b6b023f85b2","modified":1725192993775},{"_id":"themes/cactus/languages/kr.yml","hash":"651fb83991c91b13b53ed55740e5402cf0f1c5e8","modified":1725192993775},{"_id":"themes/cactus/languages/nl.yml","hash":"ac0573352ad2c737a7686bcca498b985e7bd6447","modified":1725192993775},{"_id":"themes/cactus/languages/pt-br.yml","hash":"4859aba788a050c2d5d0b997693b0c8c24b349f7","modified":1725192993775},{"_id":"themes/cactus/languages/pl.yml","hash":"8a2d6dc874d86c38d42c2c861c39590647b5d536","modified":1725192993775},{"_id":"themes/cactus/languages/ru.yml","hash":"81b57fcd1977ef534f4bf303dbc1b4710cc7f057","modified":1725192993775},{"_id":"themes/cactus/languages/tr.yml","hash":"43eb6f5abfb7f3d5a7a76af9f4f18c11182e6eb3","modified":1725192993775},{"_id":"themes/cactus/languages/ua.yml","hash":"d56eee90b599758c36e2b3437feb6515c0f512b9","modified":1725192993775},{"_id":"themes/cactus/languages/zh-CN.yml","hash":"d016060817311addb4c528de440126b975038c31","modified":1725192993775},{"_id":"themes/cactus/languages/vi.yml","hash":"f84893c3ec3e45875c90069e14b17ed3016ed973","modified":1725192993775},{"_id":"themes/cactus/languages/zh-TW.yml","hash":"3ee52bad37171900560d4082c2ceae25e6afddb3","modified":1725192993776},{"_id":"themes/cactus/layout/404.ejs","hash":"b911da998c160cceb8cd7c4dae709a1374ed2491","modified":1725192993776},{"_id":"themes/cactus/layout/index.ejs","hash":"fb73e020655ec4696eb7f91a350bf3b9c0fa2755","modified":1725192993777},{"_id":"themes/cactus/layout/archive.ejs","hash":"8e9bb1199694b229d2a3de1c3a55188a28e6f5e7","modified":1725192993777},{"_id":"themes/cactus/layout/layout.ejs","hash":"e03062a5a70d71dff9f09dd86769d1a13b33afd9","modified":1725192993777},{"_id":"themes/cactus/layout/page.ejs","hash":"c5465d5315a7544aa466b01fd8cfb62917a8bb1d","modified":1725192993777},{"_id":"themes/cactus/layout/post.ejs","hash":"f8eac342118298447ef38c21f9560a359e488395","modified":1725192993778},{"_id":"themes/cactus/scripts/meta.js","hash":"654868666b6573b2cee7e750b47ad8a3c2ee13a0","modified":1725192993778},{"_id":"themes/cactus/scripts/error_404.js","hash":"f83b290e47cb78a2754152fccc34e571a72087bd","modified":1725192993778},{"_id":"themes/cactus/scripts/page_title.js","hash":"fa662dbdb82779af1b95e35ed7ccdf4866a53dee","modified":1725192993778},{"_id":"themes/cactus/scripts/thumbnail.js","hash":"df8829fd8c3119650037eba5ec11bdce06acff9d","modified":1725192993778},{"_id":"themes/cactus/scripts/cdn.js","hash":"887edec364d51efa7c524446483188c6ad05adaf","modified":1725192993778},{"_id":"themes/cactus/scripts/merge-configs.js","hash":"2048c3415d96b17b9d84aa44bc0c25f1210525f8","modified":1725192993778},{"_id":"themes/cactus/layout/_partial/comments.ejs","hash":"4e75035a427fd137ae7f12940209e8e97845df3b","modified":1725192993776},{"_id":"themes/cactus/layout/_partial/google_analytics.ejs","hash":"64aeee0fdfc06207573ddbf8b91f6d3f007ccea9","modified":1725192993776},{"_id":"themes/cactus/layout/_partial/footer.ejs","hash":"12fd63b51472c9c5b8b7d167eb1a96bf1d686c20","modified":1725192993776},{"_id":"themes/cactus/layout/_partial/header.ejs","hash":"21d60f0bb82367cc2bee0c7eb3c06c7f5ab56a6d","modified":1725192993776},{"_id":"themes/cactus/layout/_partial/head.ejs","hash":"4709e17487315e8f0a5b38f0611f0244f16fdd87","modified":1725192993776},{"_id":"themes/cactus/layout/_partial/pagination.ejs","hash":"247bf8ec39965fe07b52f5a3d04e02e0aaf2da57","modified":1725192993776},{"_id":"themes/cactus/layout/_partial/scripts.ejs","hash":"9bbc0cf2dd0d7cdaafe827c3945a9bea1503da83","modified":1725192993777},{"_id":"themes/cactus/layout/_partial/styles.ejs","hash":"c6bc7e8a422c5bb57f88fed1d1b0694d03e24e74","modified":1725192993777},{"_id":"themes/cactus/layout/_partial/search.ejs","hash":"8b4bf9cf5db0ce762a31fc3baae0f2fc004bece4","modified":1725192993777},{"_id":"themes/cactus/layout/_partial/umami_analytics.ejs","hash":"3e79be7343c4d0a5971d3aa659e3750672a91e72","modified":1725192993777},{"_id":"themes/cactus/source/css/_extend.styl","hash":"b6a4e5905a7515dda66919167531a5ab2b3d1fe2","modified":1725192993779},{"_id":"themes/cactus/source/css/_fonts.styl","hash":"354809b5a64e8a47a66c66fd1a28ac597c1460a6","modified":1725192993779},{"_id":"themes/cactus/source/css/_mixins.styl","hash":"1a9e309523df9685e8d088dcff0a809c58e2c392","modified":1725192993785},{"_id":"themes/cactus/source/css/_util.styl","hash":"2bfeb2e2605dd5235693b00c71a212646d2e0410","modified":1725192993786},{"_id":"themes/cactus/source/css/rtl.styl","hash":"ff8700e1626feeb53d905a2df2777bda7d1eca50","modified":1725192993786},{"_id":"themes/cactus/source/css/style.styl","hash":"4ee0091bfa3cf43fa528d54df378f3d977257342","modified":1725192993786},{"_id":"themes/cactus/source/css/_variables.styl","hash":"69d9c5e95edcaee5ccd8218262b989ce721cce79","modified":1725192993786},{"_id":"themes/cactus/source/js/main.js","hash":"619ac6529d140711e3b14f739a192bb31c4824ff","modified":1725192993788},{"_id":"themes/cactus/source/images/apple-touch-icon.png","hash":"57e2def34682655f41a0be2d083f16765ba7858b","modified":1725192993786},{"_id":"themes/cactus/source/js/search.js","hash":"914a2ce72fb325106c61600200be823b72bfb39f","modified":1725192993788},{"_id":"themes/cactus/source/images/favicon-192x192.png","hash":"96e6fcbbb13a5914a6131391e210eb7dfd13d692","modified":1725192993786},{"_id":"themes/cactus/source/images/favicon.ico","hash":"189f9842bcb79a6f8f9e8445bc8bbd773443826b","modified":1725192993787},{"_id":"themes/cactus/layout/_partial/post/actions_mobile.ejs","hash":"5ee47fffaf428802cfe47dd2e6186c6bbff85067","modified":1725192993776},{"_id":"themes/cactus/layout/_partial/post/actions_desktop.ejs","hash":"54a590e39e2d6e9ee41b8a1423a7bc67ab03363d","modified":1725192993776},{"_id":"themes/cactus/layout/_partial/post/category.ejs","hash":"aeb99694d8492d4fcda320493b259fb68bf21830","modified":1725192993776},{"_id":"themes/cactus/layout/_partial/post/date.ejs","hash":"23770328c7d900ecc7fd87930dc24b095eb272ac","modified":1725192993777},{"_id":"themes/cactus/layout/_partial/post/gallery.ejs","hash":"9aecd8908e8a684f33dc20c02497c0f1774137c7","modified":1725192993777},{"_id":"themes/cactus/layout/_partial/post/tag.ejs","hash":"80bd2afd49b296e6441ab977a0614add710c32a3","modified":1725192993777},{"_id":"themes/cactus/layout/_partial/post/share.ejs","hash":"9a15a7c005cfe518fdc9ec61a5107c76012f49d0","modified":1725192993777},{"_id":"themes/cactus/source/css/_colors/classic.styl","hash":"bc09f8777a6c99030da953dfdb84f793c5e4fd85","modified":1725192993779},{"_id":"themes/cactus/layout/_partial/post/title.ejs","hash":"b2a00781d1301ff4d362b5d2a97480052ee4cae1","modified":1725192993777},{"_id":"themes/cactus/source/css/_colors/dark.styl","hash":"9aa43b1f23d5d268dfa36bd942d6ce97b7677c4d","modified":1725192993779},{"_id":"themes/cactus/source/css/_colors/light.styl","hash":"d14ef1aa02d0895b6f9321ebfc23a1ec84b054b8","modified":1725192993779},{"_id":"themes/cactus/source/css/_highlight/agate.styl","hash":"53027913ed8d4f75ac3e49e76aad824f0df62da3","modified":1725192993779},{"_id":"themes/cactus/source/css/_colors/white.styl","hash":"88e93a9d3fe1d0270d65cabdeacc18bd94d45937","modified":1725192993779},{"_id":"themes/cactus/source/css/_highlight/androidstudio.styl","hash":"2af0861725f97f0ee2ded67c3d2d4548c62b2d16","modified":1725192993779},{"_id":"themes/cactus/source/css/_highlight/arduino-light.styl","hash":"15e8572585cd708221c513dea4bdd89d8fe56c10","modified":1725192993779},{"_id":"themes/cactus/source/css/_highlight/arta.styl","hash":"b3e81e3e694ceb8deed178adb8b91013c5120e30","modified":1725192993779},{"_id":"themes/cactus/source/css/_highlight/ascetic.styl","hash":"32cff3bef6fac3760fe78f203096477052a90552","modified":1725192993779},{"_id":"themes/cactus/source/css/_highlight/atelier-cave-light.styl","hash":"a5be0744a7ecf4a08f600ade4cfd555afc67bc15","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-cave-dark.styl","hash":"ce63dd8548688d88254405eedfa75b1d7c82449e","modified":1725192993779},{"_id":"themes/cactus/source/css/_highlight/atelier-dune-dark.styl","hash":"c196ff0ee064af0e507823694ae39020addfc280","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-estuary-dark.styl","hash":"0bb16a4eff93688f40787abc2f9e56e7d5cc93e7","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-dune-light.styl","hash":"931435fbc6f974e8ce9e32722680035d248a9dc1","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-estuary-light.styl","hash":"344276ca9b27e51d4c907f76afe5d13cf8e60bdf","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-forest-dark.styl","hash":"effbc5d75fa87203c847039869c22031b40d5b7d","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-forest-light.styl","hash":"95228d9f2102fad425536aac44b80b2cba1f5950","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-heath-light.styl","hash":"8c8c2e445abef85273be966d59770e9ced6aac21","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-heath-dark.styl","hash":"9a2e9a1d0a01bbdf158560c3ed1c134e098b2c68","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-lakeside-dark.styl","hash":"10ee3882fca7b97a37bd309d2d35fce9868647bb","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-plateau-dark.styl","hash":"84c80e6f67f62fce958d25817c277d2360272617","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-savanna-dark.styl","hash":"e32c1c70def8060fce5e790979a126da650ac642","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-plateau-light.styl","hash":"d1a05fdd1ededc9063d181ab25bad55a164aeb4a","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-lakeside-light.styl","hash":"2c54cb9bdb259ae3b5b29f63ac2469ed34b08578","modified":1725192993780},{"_id":"themes/cactus/source/css/_highlight/atelier-savanna-light.styl","hash":"f8244c93711c7cb59dd79d2df966806b30d171ea","modified":1725192993781},{"_id":"themes/cactus/source/css/_highlight/atelier-seaside-dark.styl","hash":"2edf385215bbe1985b1a10106525d362667d28c2","modified":1725192993781},{"_id":"themes/cactus/source/css/_highlight/atelier-seaside-light.styl","hash":"0597342da6e2d0c5bdcc7d42dabb07322b1a4177","modified":1725192993781},{"_id":"themes/cactus/source/css/_highlight/atelier-sulphurpool-dark.styl","hash":"538a14321193cd8abf2ddc484306631e54149ffb","modified":1725192993781},{"_id":"themes/cactus/source/css/_highlight/atelier-sulphurpool-light.styl","hash":"efa52713efc468abeeb2b9299704371583b857de","modified":1725192993781},{"_id":"themes/cactus/source/css/_highlight/brown-paper.styl","hash":"c2326ba20a5020a66ca7895258d18833327d4334","modified":1725192993781},{"_id":"themes/cactus/source/css/_highlight/codepen-embed.styl","hash":"8b7b34484f76a6c2c3b1a9e49abb9b382f439ae8","modified":1725192993781},{"_id":"themes/cactus/source/css/_highlight/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1725192993781},{"_id":"themes/cactus/source/css/_highlight/dark.styl","hash":"f5e6e75958de59e87fc6be3a1668e870e20bc836","modified":1725192993781},{"_id":"themes/cactus/source/css/_highlight/color-brewer.styl","hash":"2a439d6214430e2f45dd4939b4dfe1fe1a20aa0f","modified":1725192993781},{"_id":"themes/cactus/source/css/_highlight/darkula.styl","hash":"9717efa9194837ba3fb4d762997d33075dcf8bfa","modified":1725192993781},{"_id":"themes/cactus/source/css/_highlight/docco.styl","hash":"b1c176378bb275f2e8caa759f36294e42d614bf1","modified":1725192993781},{"_id":"themes/cactus/source/css/_highlight/far.styl","hash":"aaac3028f5e33123cd123a583cddc9290c45ec8e","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/foundation.styl","hash":"bf8ddc94b4ad995b8b8805b5a4cf95004553fdac","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/github.styl","hash":"3336aeba324c6d34a6fd41fef9b47bc598f7064c","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/github-gist.styl","hash":"48211a03d33e7f7ada0b261162bea06676155a71","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/grayscale.styl","hash":"bf37d8b8d1e602126c51526f0cc28807440228ed","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/googlecode.styl","hash":"bda816beee7b439814b514e6869dc678822be1bc","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/gruvbox-dark.styl","hash":"76b744c14fd5600bea64731c05df97c2df75523f","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/highlightjs.styl","hash":"0e198b7a59191c7a39b641a4ddd22c948edb9358","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/hopscotch.styl","hash":"1378a6bc67a32c0cbff72ab771268b53f9aa586d","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/hybrid.styl","hash":"b8eb5c69d12f2ee5ebc50265ae271699d7f1a8d3","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/idea.styl","hash":"a02967cb51c16a34e0ee895d33ded2b823d35b21","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/index.styl","hash":"002d5596f6379cc87dbd43d9145bc764aa666be1","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/ir-black.styl","hash":"53e5d74326a4527b92272bbd6946d4fec92720e8","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/kimbie.styl","hash":"51b889ca7c6fe178cfbbe28d875a6ea427184441","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/kimbie.dark.styl","hash":"45dbb168f22d739d0109745d2decd66b5f94e786","modified":1725192993782},{"_id":"themes/cactus/source/css/_highlight/magula.styl","hash":"16d323f989b1420a0f72ef989242ece9bf17a456","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/kimbie.light.styl","hash":"61f8baed25be05288c8604d5070afbcd9f183f49","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/monokai-sublime.styl","hash":"c385b11345894be7e6ce3c5f08663e199933b8e4","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/mono-blue.styl","hash":"4c89a6ae29de67c0700585af82a60607e85df928","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/monokai.styl","hash":"f87be027848ea6bee623a08ad1e17b2f5b7937ee","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/obsidian.styl","hash":"199e28326be8590883f0813ebbd54fcfaa4750fd","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/paraiso-dark.styl","hash":"f1537bd868579fa018ecdbfd2eb922dcf3ba2cac","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/paraiso-light.styl","hash":"d224d1df0eb3395d9eea1344cee945c228af2911","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/paraiso.styl","hash":"75f181eece6b71d033ea0c8d6cf00ae7efb9e29b","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/pojoaque.styl","hash":"4e7b6b046b8575ac749f6aec4e953a62ada27a36","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/railscasts.styl","hash":"b6674db9210e0c4444e4835fff2d1361f3ebd64c","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/pojoaque.jpg","hash":"c5fe6533b88b21f8d90d3d03954c6b29baa67791","modified":1725192993783},{"_id":"themes/cactus/source/css/_highlight/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1725192993784},{"_id":"themes/cactus/source/css/_highlight/rainbow.styl","hash":"c0cf97aae3e10fdcd10414547a711c9effbc39b8","modified":1725192993784},{"_id":"themes/cactus/source/css/_highlight/solarized-dark.styl","hash":"90c9da5aa594383697e5b18892a7f95beb053f55","modified":1725192993784},{"_id":"themes/cactus/source/css/_highlight/school-book.styl","hash":"d43560fe519a931ce6da7d57416d7aa148441b83","modified":1725192993784},{"_id":"themes/cactus/source/css/_highlight/solarized-light.styl","hash":"aa0dd3fd25c464183b59c5575c9bee8756b397f2","modified":1725192993784},{"_id":"themes/cactus/source/css/_highlight/sunburst.styl","hash":"af3eec0fd56151e55bbd49c31b151f36717611d8","modified":1725192993784},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night-blue.styl","hash":"f24c17d0ab815dcfaab3438cb9fe2ab4839f5e0d","modified":1725192993784},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night-bright.styl","hash":"7674fecb6d27350727dc0d2dc93bc018382ebbd0","modified":1725192993784},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night.styl","hash":"16ba09b2db501e4e3e2e7d62595d9bf935bf27c4","modified":1725192993784},{"_id":"themes/cactus/source/css/_highlight/tomorrow.styl","hash":"15779cf6846725c7c35fc56cac39047d7e0aec1c","modified":1725192993784},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night-eighties.styl","hash":"28d751075ebabf7d0327a36f725076fe82fdf626","modified":1725192993784},{"_id":"themes/cactus/source/css/_highlight/xcode.styl","hash":"5e8532ae8366dcf6a4ef5e4813dc3d42ab3d0a50","modified":1725192993784},{"_id":"themes/cactus/source/css/_highlight/vs.styl","hash":"959a746f4b37aacb5d1d6ff1d57e0c045289d75d","modified":1725192993784},{"_id":"themes/cactus/source/css/_partial/archive.styl","hash":"31aef892437d5734a134c34f2a8a6610a8f671c3","modified":1725192993785},{"_id":"themes/cactus/source/css/_highlight/zenburn.styl","hash":"68ff9332ccc03f9389b15b713415cde016f8088f","modified":1725192993785},{"_id":"themes/cactus/source/css/_partial/article.styl","hash":"258370d8ab98e63804ead9bc030f633ca97a1235","modified":1725192993785},{"_id":"themes/cactus/source/css/_partial/categories.styl","hash":"a43f00e61b3507f130b8a3f8108a4eeca147c2a0","modified":1725192993785},{"_id":"themes/cactus/source/css/_partial/comments.styl","hash":"1e90f1fb9d4c155df518cacb5a537e9de9c042c1","modified":1725192993785},{"_id":"themes/cactus/source/css/_partial/footer.styl","hash":"61c2c7c5f73a0022ec41830bea0812a97f522d7c","modified":1725192993785},{"_id":"themes/cactus/source/css/_partial/header.styl","hash":"8ce12f14382b6d471e3fe1266573b34fa84deaa0","modified":1725192993785},{"_id":"themes/cactus/source/css/_partial/index.styl","hash":"59c99f4ea3a73bf47ce030df166c5e33d5de31fb","modified":1725192993785},{"_id":"themes/cactus/source/css/_partial/pagination.styl","hash":"950bf517bbe7adb9a9aa4eb5ddec74ffc7598787","modified":1725192993785},{"_id":"themes/cactus/source/css/_partial/tags.styl","hash":"d571d5c7c960300d29c5f0ec3fe1140322ecd6b3","modified":1725192993786},{"_id":"themes/cactus/source/css/_partial/search.styl","hash":"159be002780c62a77f46947cf854a7342fba24f4","modified":1725192993786},{"_id":"themes/cactus/source/lib/clipboard/clipboard.min.js","hash":"9a7cb405f9beed005891587d41f76a0720893ffc","modified":1725192993788},{"_id":"themes/cactus/source/css/_partial/tooltip.styl","hash":"2daff581ec3efaec840cbfdee512195919c32629","modified":1725192993786},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff","hash":"f6fda2de0348b3e3b7de73267f9f8e97a62f8353","modified":1725192993815},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff2","hash":"7ea4fd7dd4cd4f480af78a0e2c5849eb921b1aeb","modified":1725192993815},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff","hash":"56e632c9196fac364c66f812a3b4635dd999ad1c","modified":1725192993817},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff2","hash":"6e40d0c7669c1adbcbf034bdc459f7bed4d6676d","modified":1725192993817},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff2","hash":"50b654d916204c30987d1987abd890ef92085ae3","modified":1725192993819},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff","hash":"1c3dbf17411b1f6a6b22c2b76e9d8511586643d0","modified":1725192993819},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff","hash":"43a8aaa3fca8721dd32a5d20f7a98dfbc87c97fd","modified":1725192993821},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff2","hash":"14b3e257c51a6a11d23b2a078017ff340c9777e4","modified":1725192993821},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.woff","hash":"235889d59ddad2b1f3243ccaab7733bd713a2a21","modified":1725192993822},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.woff2","hash":"a9714ffb842afc74836e64de04b52d8c37c87c8a","modified":1725192993822},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff","hash":"c0e784de2eb5261cca244928f8a81fd893c3fe16","modified":1725192993823},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff2","hash":"9b03b1a9071709f5b7dbca13412ecef6cb7a2a67","modified":1725192993823},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.woff","hash":"2e8e6d38d361def5f48baac366f04e3db3ed4828","modified":1725192993825},{"_id":"themes/cactus/source/css/_partial/post/actions_desktop.styl","hash":"a1f36f9a3fd5ffcd832bf39e9402678978035d48","modified":1725192993785},{"_id":"themes/cactus/source/lib/vazir-font/font-face.css","hash":"ba0030e1cd28a8caa7a5bb74b98da7c7bb185c90","modified":1725192993825},{"_id":"themes/cactus/source/css/_partial/post/actions_mobile.styl","hash":"0d2966c1d870392476864af8ee3ba312ba30cb82","modified":1725192993785},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.woff2","hash":"e213bb26bc7f10e1df3fe2d03d3ecaecd6e6d371","modified":1725192993825},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.ttf","hash":"67afa6237670ab99125056f2899129f22912dcf3","modified":1725192993790},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-v4compatibility.woff2","hash":"8f80d0bbe995f7fe92320fdaec10cd5ccd710a51","modified":1725192993792},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-v4compatibility.ttf","hash":"a9d072aca9e0fadc2a7167671ce3d6b18d9cd2cc","modified":1725192993792},{"_id":"themes/cactus/source/lib/justified-gallery/css/justifiedGallery.min.css","hash":"dd3052149d3054f35efb823c68dd78e78aad5875","modified":1725192993793},{"_id":"themes/cactus/source/lib/justified-gallery/js/jquery.justifiedGallery.min.js","hash":"ad8f48b4022498078b089fcdd1e8b47faf496931","modified":1725192993793},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"fb363d27cfdfe71a243fa2ac3dab2815232b9b7e","modified":1725192993790},{"_id":"themes/cactus/source/lib/jquery/jquery.min.js","hash":"eda46747c71d38a880bee44f9a439c3858bb8f99","modified":1725192993793},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.eot","hash":"91152bd73e7ff8d943e3bde3ddb0fa0a018e1c21","modified":1725192993814},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.ttf","hash":"b65915e3fa57b5c19995d15dc2341d115c1971b9","modified":1725192993815},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.eot","hash":"5c1c680fade45393e4a5bb4548a092cd5ea6811e","modified":1725192993816},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.ttf","hash":"122bb778b17a152c426a825ee981610a4bd59bf3","modified":1725192993817},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.ttf","hash":"df82b80c4d3b11e70dcd269fc62ac97cbfa0414d","modified":1725192993818},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.eot","hash":"a059359e9bea17dc2ff2ede955a05bf0dc4d00d0","modified":1725192993818},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.eot","hash":"d9ec1f9f3fefd57e446cbe86dc297f1ff269b6de","modified":1725192993820},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.ttf","hash":"948a091f0fdb8c7ae17d5ef8e51bd8830d65dd9a","modified":1725192993821},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.eot","hash":"521c01f0eb79a48025e972ecbe21b0d7fb15437b","modified":1725192993821},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.ttf","hash":"643c28c8f8a2bce1a0d62525aa045cd9883773cd","modified":1725192993822},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.eot","hash":"a0ea0bdaef00b35544f9a21d25d35db9a79f7189","modified":1725192993822},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.ttf","hash":"6aacb0eecb03c660570b6e159ba5ca97ca7461cf","modified":1725192993823},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.eot","hash":"af46f7f4e10a1440a4c97b350622d279143e6798","modified":1725192993824},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.ttf","hash":"1e08b6373c2e086f24776df9b11e4be6bbcc8a4a","modified":1725192993824},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"4350f9ba93384634faf35f41c503c99c767f1069","modified":1725192993790},{"_id":"themes/cactus/source/lib/font-awesome/css/all.min.css","hash":"8c06d82739d14b094ff6d9036021a252bd1d985d","modified":1725192993788},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.ttf","hash":"f0982a77285d53653845b0a78170b4688db972f1","modified":1725192993790},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"6b99aa650bd12a36caa14e0127435d8f4cd3ba73","modified":1725192993792},{"_id":"themes/cactus/source/images/logo.png","hash":"0e3029251dfda26adee2761f71377297e8c26871","modified":1725192993788},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.ttf","hash":"20bd663830188cbadd2264e1daf9497c3ffc3621","modified":1725192993792},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Italic.ttf","hash":"96c97a0a098ca40802f948ae56fa37aa6683d034","modified":1725192993800},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Italic.ttf","hash":"68700db02debd4b922304134da83b829cbfddfc9","modified":1725192993809},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Italic.ttf","hash":"7f7cdbdcc26279c04046632e22d872f111bc9399","modified":1725192993811},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Bold.ttf","hash":"bfa1ed9a263ed78462f06d322de13bd5bd0906b2","modified":1725192993796},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Regular.ttf","hash":"2b912dd13f052f645ee19951604610bb350d50af","modified":1725192993803},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-BoldItalic.ttf","hash":"a9a431fc7a6c3a67c98021d4035c12a07a4f1070","modified":1725192993798},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Bold.ttf","hash":"a8a8df3393bccc365335fc5eb0a62a6b7ccd32b9","modified":1725192993804},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-BoldItalic.ttf","hash":"65ddb11e75ee93909e845ab912a36717c48f1c94","modified":1725192993806},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Regular.ttf","hash":"5e220152adefe905b2197f873d7cee99eca50e91","modified":1725192993809},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Bold.ttf","hash":"df202ce09cbdc70bc16b81983a13ef0f94e46f10","modified":1725192993810},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-BoldItalic.ttf","hash":"d895a1bd25e36c58b7f463ebe14de09f186d5ab4","modified":1725192993811},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Regular.ttf","hash":"56fa0e33a390b704afc56af93a31576ccdbbdd9e","modified":1725192993813},{"_id":"public/about/index.html","hash":"7610074a640a8fdaab6f39dce260da0f517b1163","modified":1725195999450},{"_id":"public/archives/index.html","hash":"b5b8bc7b2722255dc42f4e86a22dfca36bda9b12","modified":1725200867889},{"_id":"public/archives/2024/index.html","hash":"375b02aa9db0a9f67be21e64792cbcd57706d676","modified":1725200867889},{"_id":"public/archives/2024/09/index.html","hash":"d64b6b01f0630bc11313a1800e5f082748c6643b","modified":1725200867889},{"_id":"public/index.html","hash":"e1f75622e19cf4ac22184c4f3905f8f992056664","modified":1725200867889},{"_id":"public/404.html","hash":"e2279c99cd038b6f75ce469da32cb63dc6a9995a","modified":1725195999450},{"_id":"public/2024/09/01/hello-world/index.html","hash":"cf1599414a8f8165eeaa06face0764db5a5f6d14","modified":1725195999450},{"_id":"public/images/apple-touch-icon.png","hash":"57e2def34682655f41a0be2d083f16765ba7858b","modified":1725193122287},{"_id":"public/images/favicon-192x192.png","hash":"96e6fcbbb13a5914a6131391e210eb7dfd13d692","modified":1725193122287},{"_id":"public/images/favicon.ico","hash":"189f9842bcb79a6f8f9e8445bc8bbd773443826b","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Black.woff","hash":"f6fda2de0348b3e3b7de73267f9f8e97a62f8353","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Black.woff2","hash":"7ea4fd7dd4cd4f480af78a0e2c5849eb921b1aeb","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Bold.woff2","hash":"6e40d0c7669c1adbcbf034bdc459f7bed4d6676d","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Light.woff2","hash":"50b654d916204c30987d1987abd890ef92085ae3","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Bold.woff","hash":"56e632c9196fac364c66f812a3b4635dd999ad1c","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Light.woff","hash":"1c3dbf17411b1f6a6b22c2b76e9d8511586643d0","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Medium.woff2","hash":"14b3e257c51a6a11d23b2a078017ff340c9777e4","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Regular.woff","hash":"235889d59ddad2b1f3243ccaab7733bd713a2a21","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Medium.woff","hash":"43a8aaa3fca8721dd32a5d20f7a98dfbc87c97fd","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Regular.woff2","hash":"a9714ffb842afc74836e64de04b52d8c37c87c8a","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Thin.woff2","hash":"9b03b1a9071709f5b7dbca13412ecef6cb7a2a67","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Thin.woff","hash":"c0e784de2eb5261cca244928f8a81fd893c3fe16","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Variable.woff2","hash":"e213bb26bc7f10e1df3fe2d03d3ecaecd6e6d371","modified":1725193122287},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.ttf","hash":"67afa6237670ab99125056f2899129f22912dcf3","modified":1725193122287},{"_id":"public/lib/font-awesome/webfonts/fa-v4compatibility.woff2","hash":"8f80d0bbe995f7fe92320fdaec10cd5ccd710a51","modified":1725193122287},{"_id":"public/lib/font-awesome/webfonts/fa-v4compatibility.ttf","hash":"a9d072aca9e0fadc2a7167671ce3d6b18d9cd2cc","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Variable.woff","hash":"2e8e6d38d361def5f48baac366f04e3db3ed4828","modified":1725193122287},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"fb363d27cfdfe71a243fa2ac3dab2815232b9b7e","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Black.eot","hash":"91152bd73e7ff8d943e3bde3ddb0fa0a018e1c21","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Black.ttf","hash":"b65915e3fa57b5c19995d15dc2341d115c1971b9","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Bold.eot","hash":"5c1c680fade45393e4a5bb4548a092cd5ea6811e","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Bold.ttf","hash":"122bb778b17a152c426a825ee981610a4bd59bf3","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Light.eot","hash":"a059359e9bea17dc2ff2ede955a05bf0dc4d00d0","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Light.ttf","hash":"df82b80c4d3b11e70dcd269fc62ac97cbfa0414d","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Medium.eot","hash":"d9ec1f9f3fefd57e446cbe86dc297f1ff269b6de","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Regular.eot","hash":"521c01f0eb79a48025e972ecbe21b0d7fb15437b","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Medium.ttf","hash":"948a091f0fdb8c7ae17d5ef8e51bd8830d65dd9a","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Regular.ttf","hash":"643c28c8f8a2bce1a0d62525aa045cd9883773cd","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Variable.eot","hash":"af46f7f4e10a1440a4c97b350622d279143e6798","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Thin.ttf","hash":"6aacb0eecb03c660570b6e159ba5ca97ca7461cf","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Variable.ttf","hash":"1e08b6373c2e086f24776df9b11e4be6bbcc8a4a","modified":1725193122287},{"_id":"public/lib/vazir-font/Vazir-Thin.eot","hash":"a0ea0bdaef00b35544f9a21d25d35db9a79f7189","modified":1725193122287},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"4350f9ba93384634faf35f41c503c99c767f1069","modified":1725193122287},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.ttf","hash":"f0982a77285d53653845b0a78170b4688db972f1","modified":1725193122287},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"6b99aa650bd12a36caa14e0127435d8f4cd3ba73","modified":1725193122287},{"_id":"public/css/rtl.css","hash":"9589fac02a34fd9084f805f801889028756bbb65","modified":1725193122287},{"_id":"public/lib/clipboard/clipboard.min.js","hash":"9a7cb405f9beed005891587d41f76a0720893ffc","modified":1725193122287},{"_id":"public/js/main.js","hash":"619ac6529d140711e3b14f739a192bb31c4824ff","modified":1725193122287},{"_id":"public/js/search.js","hash":"914a2ce72fb325106c61600200be823b72bfb39f","modified":1725193122287},{"_id":"public/lib/vazir-font/font-face.css","hash":"ba0030e1cd28a8caa7a5bb74b98da7c7bb185c90","modified":1725193122287},{"_id":"public/lib/justified-gallery/css/justifiedGallery.min.css","hash":"dd3052149d3054f35efb823c68dd78e78aad5875","modified":1725193122287},{"_id":"public/css/style.css","hash":"775c8fa6860f020f1bd5ebdc2d8346998602138a","modified":1725193122287},{"_id":"public/images/logo.png","hash":"0e3029251dfda26adee2761f71377297e8c26871","modified":1725193122287},{"_id":"public/lib/justified-gallery/js/jquery.justifiedGallery.min.js","hash":"ad8f48b4022498078b089fcdd1e8b47faf496931","modified":1725193122287},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.ttf","hash":"20bd663830188cbadd2264e1daf9497c3ffc3621","modified":1725193122287},{"_id":"public/lib/jquery/jquery.min.js","hash":"eda46747c71d38a880bee44f9a439c3858bb8f99","modified":1725193122287},{"_id":"public/lib/meslo-LG/MesloLGM-Italic.ttf","hash":"68700db02debd4b922304134da83b829cbfddfc9","modified":1725193122287},{"_id":"public/lib/meslo-LG/MesloLGS-Italic.ttf","hash":"7f7cdbdcc26279c04046632e22d872f111bc9399","modified":1725193122287},{"_id":"public/lib/meslo-LG/MesloLGL-Italic.ttf","hash":"96c97a0a098ca40802f948ae56fa37aa6683d034","modified":1725193122287},{"_id":"public/lib/font-awesome/css/all.min.css","hash":"8c06d82739d14b094ff6d9036021a252bd1d985d","modified":1725193122287},{"_id":"public/lib/meslo-LG/MesloLGL-Bold.ttf","hash":"bfa1ed9a263ed78462f06d322de13bd5bd0906b2","modified":1725193122287},{"_id":"public/lib/meslo-LG/MesloLGL-Regular.ttf","hash":"2b912dd13f052f645ee19951604610bb350d50af","modified":1725193122287},{"_id":"public/lib/meslo-LG/MesloLGM-Bold.ttf","hash":"a8a8df3393bccc365335fc5eb0a62a6b7ccd32b9","modified":1725193122287},{"_id":"public/lib/meslo-LG/MesloLGM-BoldItalic.ttf","hash":"65ddb11e75ee93909e845ab912a36717c48f1c94","modified":1725193122287},{"_id":"public/lib/meslo-LG/MesloLGL-BoldItalic.ttf","hash":"a9a431fc7a6c3a67c98021d4035c12a07a4f1070","modified":1725193122287},{"_id":"public/lib/meslo-LG/MesloLGS-BoldItalic.ttf","hash":"d895a1bd25e36c58b7f463ebe14de09f186d5ab4","modified":1725193122287},{"_id":"public/lib/meslo-LG/MesloLGM-Regular.ttf","hash":"5e220152adefe905b2197f873d7cee99eca50e91","modified":1725193122287},{"_id":"public/lib/meslo-LG/MesloLGS-Bold.ttf","hash":"df202ce09cbdc70bc16b81983a13ef0f94e46f10","modified":1725193122287},{"_id":"public/lib/meslo-LG/MesloLGS-Regular.ttf","hash":"56fa0e33a390b704afc56af93a31576ccdbbdd9e","modified":1725193122287},{"_id":"source/writing/index.md","hash":"c3f971376908720b935c3f4a451978904cca76f6","modified":1725193747852},{"_id":"public/writing/index.html","hash":"dd7274a886188434239c1255b40cb64c0e874911","modified":1725194756134},{"_id":"source/_posts/LLM_Illusion.md","hash":"50a7837c2e6b12b530b718fa1d675cac4d855b3d","modified":1725193982750},{"_id":"public/2024/09/01/LLM_Illusion/index.html","hash":"bed634e5f4526f4bd3b6617c484aff12ec69caf4","modified":1725193987043},{"_id":"source/tags/index.md","hash":"2f952cb98176290c444bebe3b247e124fddbd438","modified":1725194528289},{"_id":"source/_posts/LlmIllusion.md","hash":"9b4b3428cf02a5ea8daf05a7d1376f74e742829b","modified":1725202427932},{"_id":"source/categories/index.md","hash":"7796053c5cb79ffe5891e22d3c9f29db062a85e4","modified":1725194525180},{"_id":"public/categories/index.html","hash":"2db26d26c721cc9bad0612eb82cee481bb8e86eb","modified":1725195999450},{"_id":"public/tags/index.html","hash":"1ff1c138668eeba8b57e2dc315606d396a09cb5d","modified":1725195999450},{"_id":"public/categories/大模型相关/index.html","hash":"c00f44354c80e04dd6366f39bdb3859766a42743","modified":1725195999450},{"_id":"public/tags/大模型/index.html","hash":"1444466d430d3ea1ba9ce80dbaeb9e2f708f1036","modified":1725195999450},{"_id":"public/2024/09/01/LlmIllusion/index.html","hash":"08e7d92c6a3fba03904a81e1ee3ab5c2cfebc940","modified":1725202458504},{"_id":"themes/cactus/.DS_Store","hash":"57f338f89e82ab50246ec87484902584e7d30ba8","modified":1725195328886},{"_id":"themes/cactus/source/.DS_Store","hash":"7ba99373793f1ddaa362db7c0cde454baa7eb4a0","modified":1725195328882},{"_id":"themes/cactus/source/images/mylogo.jpg","hash":"69aa5e45c21d39e38556088214b427cdb2e51c73","modified":1725195544434},{"_id":"themes/cactus/source/images/mylogo2.jpg","hash":"1f1e90b663c677c5d7a13c9965fdac8d548200c1","modified":1725195312000},{"_id":"public/images/mylogo.jpg","hash":"69aa5e45c21d39e38556088214b427cdb2e51c73","modified":1725195627085},{"_id":"public/images/mylogo2.jpg","hash":"1f1e90b663c677c5d7a13c9965fdac8d548200c1","modified":1725195627085},{"_id":"themes/cactus/source/images/myfavicon.ico","hash":"9920ab48ec4ee1b9cfe4b8149b08d01668f0981b","modified":1725195791631},{"_id":"public/images/myfavicon.ico","hash":"9920ab48ec4ee1b9cfe4b8149b08d01668f0981b","modified":1725195999450},{"_id":"source/_posts/大模型幻觉 35da3f14f38142639d51237c175244e0/image 4.png","hash":"e6bcc2a260dfa42a03912eb60055fc7aab5e786a","modified":1725169074000},{"_id":"source/_posts/大模型幻觉 35da3f14f38142639d51237c175244e0/image 3.png","hash":"d5b2232d3e789605bbd2827be9419c6743d625f9","modified":1725169074000},{"_id":"source/_posts/大模型幻觉 35da3f14f38142639d51237c175244e0/image 1.png","hash":"394b2288127f142628d2d25117f92341d621e1a6","modified":1725169074000},{"_id":"source/_posts/大模型幻觉 35da3f14f38142639d51237c175244e0/image 2.png","hash":"76bc56571c93b53b44f0c32aabe032d89bcb8e85","modified":1725169074000},{"_id":"source/_posts/大模型幻觉 35da3f14f38142639d51237c175244e0/image 5.png","hash":"65e0a4aab49e4d24007395d4d8492797ae11b4e8","modified":1725169074000},{"_id":"source/_posts/大模型幻觉 35da3f14f38142639d51237c175244e0/image.png","hash":"acbf05931f3c29bde3fb1da8328714ee84134bd0","modified":1725169074000},{"_id":"source/_posts/大模型幻觉 35da3f14f38142639d51237c175244e0/image 6.png","hash":"5a80697dfba9a9b447763bd2c384d4da9835859e","modified":1725169074000},{"_id":"source/_posts/llmillusion_source/image 3.png","hash":"d5b2232d3e789605bbd2827be9419c6743d625f9","modified":1725169074000},{"_id":"source/_posts/llmillusion_source/image 4.png","hash":"e6bcc2a260dfa42a03912eb60055fc7aab5e786a","modified":1725169074000},{"_id":"source/_posts/llmillusion_source/image 2.png","hash":"76bc56571c93b53b44f0c32aabe032d89bcb8e85","modified":1725169074000},{"_id":"source/_posts/llmillusion_source/image 5.png","hash":"65e0a4aab49e4d24007395d4d8492797ae11b4e8","modified":1725169074000},{"_id":"source/_posts/llmillusion_source/image 1.png","hash":"394b2288127f142628d2d25117f92341d621e1a6","modified":1725169074000},{"_id":"source/_posts/llmillusion_source/image.png","hash":"acbf05931f3c29bde3fb1da8328714ee84134bd0","modified":1725169074000},{"_id":"source/_posts/llmillusion_source/image 6.png","hash":"5a80697dfba9a9b447763bd2c384d4da9835859e","modified":1725169074000},{"_id":"source/_posts/image.png","hash":"acbf05931f3c29bde3fb1da8328714ee84134bd0","modified":1725169074000},{"_id":"public/2024/09/01/hello-world/source/image 3.png","hash":"d5b2232d3e789605bbd2827be9419c6743d625f9","modified":1725199460241},{"_id":"public/2024/09/01/hello-world/source/image 4.png","hash":"e6bcc2a260dfa42a03912eb60055fc7aab5e786a","modified":1725199460241},{"_id":"public/2024/09/01/hello-world/source/image 1.png","hash":"394b2288127f142628d2d25117f92341d621e1a6","modified":1725199460241},{"_id":"public/2024/09/01/hello-world/source/image 6.png","hash":"5a80697dfba9a9b447763bd2c384d4da9835859e","modified":1725199460241},{"_id":"public/2024/09/01/hello-world/source/image 5.png","hash":"65e0a4aab49e4d24007395d4d8492797ae11b4e8","modified":1725199460241},{"_id":"public/2024/09/01/hello-world/source/image 2.png","hash":"76bc56571c93b53b44f0c32aabe032d89bcb8e85","modified":1725199460241},{"_id":"public/2024/09/01/hello-world/source/image.png","hash":"acbf05931f3c29bde3fb1da8328714ee84134bd0","modified":1725199460241},{"_id":"source/_posts/posts/image 3.png","hash":"d5b2232d3e789605bbd2827be9419c6743d625f9","modified":1725169074000},{"_id":"source/_posts/posts/image 4.png","hash":"e6bcc2a260dfa42a03912eb60055fc7aab5e786a","modified":1725169074000},{"_id":"source/_posts/posts/image 1.png","hash":"394b2288127f142628d2d25117f92341d621e1a6","modified":1725169074000},{"_id":"source/_posts/posts/image 2.png","hash":"76bc56571c93b53b44f0c32aabe032d89bcb8e85","modified":1725169074000},{"_id":"source/_posts/posts/image 5.png","hash":"65e0a4aab49e4d24007395d4d8492797ae11b4e8","modified":1725169074000},{"_id":"source/_posts/posts/image.png","hash":"acbf05931f3c29bde3fb1da8328714ee84134bd0","modified":1725169074000},{"_id":"source/_posts/posts/image 6.png","hash":"5a80697dfba9a9b447763bd2c384d4da9835859e","modified":1725169074000},{"_id":"public/2024/09/01/hello-world/3.png","hash":"d5b2232d3e789605bbd2827be9419c6743d625f9","modified":1725199634980},{"_id":"public/2024/09/01/hello-world/4.png","hash":"e6bcc2a260dfa42a03912eb60055fc7aab5e786a","modified":1725199634980},{"_id":"public/2024/09/01/hello-world/1.png","hash":"394b2288127f142628d2d25117f92341d621e1a6","modified":1725199634980},{"_id":"public/2024/09/01/hello-world/2.png","hash":"76bc56571c93b53b44f0c32aabe032d89bcb8e85","modified":1725199634980},{"_id":"public/2024/09/01/hello-world/png","hash":"acbf05931f3c29bde3fb1da8328714ee84134bd0","modified":1725199634980},{"_id":"public/2024/09/01/hello-world/5.png","hash":"65e0a4aab49e4d24007395d4d8492797ae11b4e8","modified":1725199634980},{"_id":"public/2024/09/01/hello-world/6.png","hash":"5a80697dfba9a9b447763bd2c384d4da9835859e","modified":1725199634980},{"_id":"source/_posts/test-pict.md","hash":"4159d1ed7e973f314b7adba1a7dc52ee4013d5b5","modified":1725202315835},{"_id":"source/_posts/LlmIllusion/image 4.png","hash":"e6bcc2a260dfa42a03912eb60055fc7aab5e786a","modified":1725169074000},{"_id":"source/_posts/LlmIllusion/image 3.png","hash":"d5b2232d3e789605bbd2827be9419c6743d625f9","modified":1725169074000},{"_id":"source/_posts/LlmIllusion/image 1.png","hash":"394b2288127f142628d2d25117f92341d621e1a6","modified":1725169074000},{"_id":"source/_posts/LlmIllusion/image 2.png","hash":"76bc56571c93b53b44f0c32aabe032d89bcb8e85","modified":1725169074000},{"_id":"source/_posts/LlmIllusion/image 5.png","hash":"65e0a4aab49e4d24007395d4d8492797ae11b4e8","modified":1725169074000},{"_id":"source/_posts/LlmIllusion/image.png","hash":"acbf05931f3c29bde3fb1da8328714ee84134bd0","modified":1725169074000},{"_id":"source/_posts/LlmIllusion/image 6.png","hash":"5a80697dfba9a9b447763bd2c384d4da9835859e","modified":1725169074000},{"_id":"public/2024/09/01/test-pict/index.html","hash":"af837012d32cd37bfacf625e4a94c143ed9b1943","modified":1725202458504},{"_id":"public/2024/09/01/hello-world/image 3.png","hash":"d5b2232d3e789605bbd2827be9419c6743d625f9","modified":1725199900439},{"_id":"public/2024/09/01/hello-world/image 4.png","hash":"e6bcc2a260dfa42a03912eb60055fc7aab5e786a","modified":1725199900439},{"_id":"public/2024/09/01/hello-world/image 1.png","hash":"394b2288127f142628d2d25117f92341d621e1a6","modified":1725199900439},{"_id":"public/2024/09/01/hello-world/image 2.png","hash":"76bc56571c93b53b44f0c32aabe032d89bcb8e85","modified":1725199900439},{"_id":"public/2024/09/01/hello-world/image 5.png","hash":"65e0a4aab49e4d24007395d4d8492797ae11b4e8","modified":1725199900439},{"_id":"public/2024/09/01/hello-world/image 6.png","hash":"5a80697dfba9a9b447763bd2c384d4da9835859e","modified":1725199900439},{"_id":"public/2024/09/01/hello-world/image.png","hash":"acbf05931f3c29bde3fb1da8328714ee84134bd0","modified":1725199900439},{"_id":"source/_posts/test-pict/image.png","hash":"acbf05931f3c29bde3fb1da8328714ee84134bd0","modified":1725169074000},{"_id":"public/2024/09/01/hello-world/age.png","hash":"acbf05931f3c29bde3fb1da8328714ee84134bd0","modified":1725200067937},{"_id":"source/_posts/LlmIllusion/image1.png","hash":"394b2288127f142628d2d25117f92341d621e1a6","modified":1725169074000},{"_id":"source/_posts/LlmIllusion/image2.png","hash":"76bc56571c93b53b44f0c32aabe032d89bcb8e85","modified":1725169074000},{"_id":"public/2024/09/01/hello-world/image2.png","hash":"76bc56571c93b53b44f0c32aabe032d89bcb8e85","modified":1725202458504},{"_id":"public/2024/09/01/hello-world/image1.png","hash":"394b2288127f142628d2d25117f92341d621e1a6","modified":1725202458504}],"Category":[{"name":"大模型相关","_id":"cm0jk8pk200038uio0tsxe9i7"}],"Data":[],"Page":[{"title":"about","date":"2024-09-01T12:18:03.000Z","_content":"🎨 **你好!我是Macvh或者Blond（**没什么特殊含义，单纯觉得这几个字母组成的单词“长”的比较帅**）**。目前在**北京**从事**大语言模型SFT**的相关工作。我有**女朋友**，一只**猫**和一只**兔子**，是个普通但不甘于平凡的人，热衷于追求一些在技术上很**“帅”**的事情，我记录这个博客是用于自我反省，并逼迫自己进行深入的思考，将零散、浅薄的想法总结、沉淀下来。\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2024-09-01 20:18:03\n---\n🎨 **你好!我是Macvh或者Blond（**没什么特殊含义，单纯觉得这几个字母组成的单词“长”的比较帅**）**。目前在**北京**从事**大语言模型SFT**的相关工作。我有**女朋友**，一只**猫**和一只**兔子**，是个普通但不甘于平凡的人，热衷于追求一些在技术上很**“帅”**的事情，我记录这个博客是用于自我反省，并逼迫自己进行深入的思考，将零散、浅薄的想法总结、沉淀下来。\n","updated":"2024-09-01T12:33:02.752Z","path":"about/index.html","_id":"cm0jjfwt300004iio811fbdqi","comments":1,"layout":"page","content":"<p>🎨 <strong>你好!我是Macvh或者Blond（</strong>没什么特殊含义，单纯觉得这几个字母组成的单词“长”的比较帅<strong>）</strong>。目前在<strong>北京</strong>从事<strong>大语言模型SFT</strong>的相关工作。我有<strong>女朋友</strong>，一只<strong>猫</strong>和一只<strong>兔子</strong>，是个普通但不甘于平凡的人，热衷于追求一些在技术上很<strong>“帅”</strong>的事情，我记录这个博客是用于自我反省，并逼迫自己进行深入的思考，将零散、浅薄的想法总结、沉淀下来。</p>\n","excerpt":"","more":"<p>🎨 <strong>你好!我是Macvh或者Blond（</strong>没什么特殊含义，单纯觉得这几个字母组成的单词“长”的比较帅<strong>）</strong>。目前在<strong>北京</strong>从事<strong>大语言模型SFT</strong>的相关工作。我有<strong>女朋友</strong>，一只<strong>猫</strong>和一只<strong>兔子</strong>，是个普通但不甘于平凡的人，热衷于追求一些在技术上很<strong>“帅”</strong>的事情，我记录这个博客是用于自我反省，并逼迫自己进行深入的思考，将零散、浅薄的想法总结、沉淀下来。</p>\n"},{"title":"categories","date":"2024-09-01T12:38:37.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2024-09-01 20:38:37\ntype: categories\n---\n","updated":"2024-09-01T12:42:05.180Z","path":"categories/index.html","_id":"cm0jk8pjk00008uioeb2qeq95","comments":1,"layout":"page","content":"","excerpt":"","more":""},{"title":"tags","date":"2024-09-01T12:37:45.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2024-09-01 20:37:45\ntype: tags\n---\n","updated":"2024-09-01T12:42:08.289Z","path":"tags/index.html","_id":"cm0jk8pjn00018uio4qr8aqva","comments":1,"layout":"page","content":"","excerpt":"","more":""}],"Post":[{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2024-09-01T12:15:25.301Z","updated":"2024-09-01T12:15:25.301Z","comments":1,"layout":"post","photos":[],"_id":"cm0jjfwt500014iio16bic1qh","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"大模型幻觉","_content":"# 大模型幻觉\n\n## 一、什么是 大模型幻觉问题？\n\n### 1.1 大模型幻觉问题定义\n\n- 定义：当模型生成的**文本不遵循原文（Faithfulness）或者不符合事实（Factualness）**，我们就可以认为模型出现了幻觉的问题。\n\n### 1.2 何为 Faithfulness and Factualness？\n\n- Faithfulness：是否遵循input content；\n- Factualness：是否符合世界知识；\n\n![](image.png)\n\n（参考：Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models，[https://arxiv.org/abs/2309.01219](https://arxiv.org/abs/2309.01219)）\n\n![](image1.png)\n\n1. **输入相冲突的幻觉。**输入相冲突的幻觉，即LLM生成的内容与用户提供的源输入相背离。当LLM生成的内容偏离用户输入时，就会产生这类幻觉。\n2. **语境冲突性幻觉。**LLM生成的内容与之前生成的信息本身相冲突。LLMs在生成冗长或多轮回答时可能会表现出自我矛盾。这种类型的幻觉产生于LLMs在整个对话过程中失去对上下文的跟踪或无法保持一致性时，这可能是由于他们在保持长期记忆或识别相关上下文方面的局限性造成的。\n3. **与事实相冲突的幻觉。**与事实相冲突的幻觉指的是LLM生成的内容不忠实于既定的世界知识。事实冲突型幻觉。当LLM生成的信息或文本与已有的世界知识相矛盾时，就会出现这种类型的幻觉。如图2所示，事实冲突幻觉的来源可能多种多样，并在LLM生命周期的不同阶段出现。\n\n### 除幻觉外的常见问题:\n\n![](image2.png)\n\n1. **歧义性：**答案可能并不一定是不正确的，但是对于用户问题却没有给出一个有用的答案。\n2. **不完整性：**不完整性问题发生在生成的响应不完整或碎片化的情况下，如上图例子中只给出了换轮胎4个步骤中的2步。\n3. **偏见：**LLMs中的偏见是指生成文本中不公平或偏见态度的表现，这些偏见可能来源于训练数据。如上图例子中将教师描述为女性是一种偏见。\n4. **信息不足：**指LLMs逃避回答某些问题的倾向，即使其有能力这样做。例如，由于奖励模型的不完善，RLHF可能会导致LLMs的过度优化，从而可能导致模型输出信息不足。\n\n### 1.3 传统任务中的模型幻觉 vs LLMs 中模型幻觉\n\n1. **在传统任务里，幻觉大都是指的是Faithfulness**：\n   1. **Intrinsic Hallucination（信息冲突）**: LMs在生成回复时，与输入信息产生了冲突，例如摘要问题里，abstract和document的信息不一致；\n   2. **Extrinsic Hallucination（无中生有）**: LMs在生成回复时，输出一些并没有体现在输入中的额外信息，比如邮箱地址、电话号码、住址，并且难以验证其真假。（PS: 按照此定义，Extrinsic Hallucination有可能是真的信息，只是需要外部信息源进行认证）\n2. **而面向LLMs，我们通常考虑的幻觉则是Factualness**：\n   1. 因为我们应用LLM的形式是open-domain Chat，而不是局限于特定任务，所以数据源可以看做任意的世界知识。LLMs如果生成了不在input source里的额外信息，但是符合事实的，这种情况也可能是对我们有帮助的。\n\n## 二、大模型幻觉从何而来？\n\n### 2.1 从 数据角度 进行分析\n\n在 数据构建过程中，由于以下问题，导致 模型幻觉 的 发生：\n\n1. 训练数据可信度问题。由于 大模型 的 训练数据 都是 通过 众包/爬虫检索 方式 收集得到的，这种数据构建方式的优点是量比较大，但是缺点是 包含 大量虚假信息。这种虚假信息 直接导致的问题就是使 模型出现错误认知；\n2. 重复数据问题。过多的重复信息也可能导致模型的知识记忆出现bias，从而导致幻觉；\n3. LLM偏向于肯定测试的样本，即人类生成的语料中也存在幻觉(可反映为过时的、双重的或捏造的表达)，LLMs很容易复制甚至放大这种幻觉行为。\n\n> 引用至 [3] Deduplicating training data makes language models better\n\n### 2.2 从 模型角度 进行分析\n\n不止是 数据角度问题，大模型幻觉问题 出现的原因 还 表现在 模型角度。\n\n- **模型结构**：如果是较弱的backbone（比如RNN）可能导致比较严重的幻觉问题，但在LLMs时代应该不太可能存在这一问题；\n- **解码算法**：\n  - 研究表明，**如果使用不确定性较高的采样算法（e.g.，top-p）会诱导LMs出现更严重的幻觉问题**。甚至可以故意在解码算法中加入一些随机性，进一步让LMs胡编乱造（可以用该方法生成一些negative samples）\n  - 局部最优化(标记预测)并不一定能确保全局最优化(序列预测)，早期的局部预测可能会将LLMs带入难以形成正确反应的境地。\n\n引用至 [4] Factuality enhanced language models for open-ended text generation\n\n- **暴露偏差**：**训练和测试阶段不匹配的exposure bias问题可能导致LLMs出现幻觉，特别是生成long-form response的时候**。\n\n> 引用至 [5] On exposure bias, hallucination and domain shift in neural machine translation\n\n- **参数知识**：**LMs在预训练阶段记忆的错误的知识，将会严重导致幻觉问题**。\n\n> 引用至 [6] Entity-based knowledge conflicts in question answering\n\n## 三、如何 评估 大模型幻觉问题？\n\n**现有的传统幻觉评估指标和人类结果的相关性往往较低**，同时大都是task-specific的 [7]。\n\n### 3.1 Reference-based\n\nReference-based的指标有两类：\n\n1. **基于Source Information和Target Reference**：利用一些统计学指标，比如**ROUGE、BLEU来评估输出结果和Source/Target信息的重叠度**;\n2. **基于Source Information**：由于NLG任务里，Target输出往往是多种多样的，因此许多工作**只基于Source信息进行幻觉的评估**。比如Knowledge F1。\n\n基于Reference的评价指标，**基本上只能评价Faithfulness，而无法评价Factualness，因此通常不适用于LLMs**。\n\n### 3.2 Reference-Free\n\n### 3.2.1 基于IE\n\n- 介绍：**将知识限定于可以用三元组形式表示的关系和事件**，基于额外的IE模型进行抽取，接着使用额外模型进行验证；\n- 缺点：\n  - 可能存在IE模型的错误传播问题；\n  - 知识被限定在三元组形式。\n\n### 3.2.2 基于QA\n\n- 介绍：\n\n1. 第一步先**基于LM生成的回复**，使用一个QG(question generation)模型生成一系列QA pairs；\n2. 第二步**给定Source Information，让QA模型对上一步生成的Question进行回复**；\n3. 第三步则是**通过对比第一步的answers和第二步的answers，计算匹配指标，衡量模型的幻觉问题**；\n\n- 缺点：可能存在IE模型的错误传播问题；难以评估Factualness，因为上述第二步里面，Source Information不可能包含全部的世界知识，因此对于一些问题难以生成可靠的回复。\n\n> 引用至 [8] FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization\n\n### 3.2.3 基于NLI\n\n- 介绍：基于NLI的方法**通过NLI模型评估是否Source Information可以蕴含Generated Text，从而评估是否出现了幻觉现象**。\n  - 缺点：\n    - Off-the-shelf NLI模型用于核查事实效果不是很好\n\n> 引用至 [9] Evaluating groundedness in dialogue systems: The BEGIN benchmark.\n\n- 无法评估需要世界知识的幻觉问题：\n  - **仅能依赖于Source进行核查**；\n  - 都是sentence-level的，**无法支撑更细粒度的幻觉检查；**\n\n> 引用至 [10] Evaluating factuality in generation with dependency-level entailment.\n\n- 幻觉问题和蕴含问题实际并不等价：\n\n1. 例子：Putin is president. -> Putin is U.S. president (可以蕴含，但是是幻觉)\n\n### 3.2.4 基于Factualness Classification Metric\n\n- 介绍：标注/构造一批和幻觉/事实有关的数据，训练检测模型，利用该模型评估新生成文本的幻觉/事实问题。\n\n> 引用至 [11] Knowledge-powered conversational agents\n\n### 3.2.5 人工评估\n\n- 介绍：目前为止最靠谱的，此外还可以依靠LLM打分（比如利用GPT4，但是GPT4也存在着严重的幻觉问题，即使经过retrival-augment，检索回来的信息也有可能是错误的）\n\n## 四、如何 缓解 大模型幻觉问题？\n\n### 4.1预训练阶段\n\nLLM的知识大多是在预训练阶段获得的。在预训练语料库中存在诸如错误信息之类的噪声数据可能会破坏LLMs的参数知识，而这是导致误差的一个重要因素。有时，还可以将语言模型获得的事实知识追溯到其训练数据。**因此，减少幻觉的直观方法可以是人工或自动整理预训练语料库，尽可能减少无法验证或不可靠的数据。**\n\n1. **人工消除噪声训练数据**\n   1. 专注于\"数据到文本\"(data-to-text)任务，并邀请人工根据给定的知识库手动编写干净准确的回复。\n   2. 在现有的表格到文本数据集中对文本进行人工提炼这一过程也大大减少了事实幻觉。\n   3. 在构建表对文训练数据时，指导注释者修改维基百科中已验证的句子，而不是直接创建新句子。\n2. **自动选择可靠数据或过滤掉噪声数据**\n   1. **GPT-3**的预训练数据就是通过与一系列高质量参考数据的相似性进行清理。\n   2. **Falcon**通过启发式规则从网络中仔细提取高质量数据，并证明了经过适当分级的相关语料库可以产生强大的LLM。为了减少幻觉，目前的LLM通常会从可靠的文本来源收集预训练数据。\n   3. **Llama2**在构建预训练语料库时，从维基百科等高度事实性的来源中向上抽取数据\n   4. 在事实性文档的句子前加上主题前缀，使每个句子在预训练时都能作为一个独立的事实，将文档名称作为主题前缀，并观察到这种方法提高了LLM在**TruthfulQA**上的性能**。**\n\n### 4.2 SFT阶段\n\n![](image%203.png)\n\n当前的LLMs一般都会经历一个被称为\"监督微调\"(SFT)的过程，以从预训练中获取所需的知识，并学习如何与用户互动。SFT通常包括首先注释或收集海量任务指令跟踪数据，然后使用极大似然法(MLE)在这些数据上对预先训练的基础LLM进行微调。\n\n**目的**\n\n- 学习如何与用户进行有效交互。\n- 从预训练中获得的知识中抽取信息。\n\n1. **加工整理数据**\n   1. 与预训练类似，一种可能的方法是要减少SFT阶段的幻觉，可以对训练数据进行整理，SFT数据量相对较小。\n   2. 手动和自动整理都是可行的选择。在未经编辑的数据上进行微调的LLM相比，在这些经过编辑的指令数据上进行微调的LLM表现出更高的真实性和事实性水平。\n2. **加入拒答**\n   1. 在推理过程中，如果被要求回答与未学知识相关的问题，他们很可能会自信地产生幻觉。\n   2. 采用以诚实为导向的SFT，即在SFT数据中引入一些诚实样本。诚实样本指的是承认自己无能的回答，如\"对不起，我不知道\"。学会拒绝回答特定问题，从而帮助减少幻觉。\n\n### 4.3 RLHF期间的缓解\n\n![](image%204.png)\n\n1. 利用人类反馈不仅能缩小机器生成的内容与人类偏好之间的差距，还能帮助LLM符合预期的标准或目标。目前常用的一个标准是\"3H\"，即有益、诚实和无害。这里的\"诚实\"只是指在LLM反应中尽量减少幻觉。\n2. 强化学习可以引导LLM探索其知识边界，使他们能够拒绝回答超出其能力范围的问题，而不是编造不真实的回答。但这种方法也带来了独特的挑战。例如，由于在乐于助人和诚实之间的权衡失衡，经过RL调整的LLM可能会表现出过度保守。\n3. **GPT4使用合成幻觉数据来训练奖励模型并执行R**L，从而将TruthfulQA的准确率从约30%提高到60%。\n4. 设计一种专门用于减轻幻觉的特殊奖励函数:\"Unhedged/Hedged Correct/Wrong\"，指的是LLM用肯定或犹豫的语气提供正确或错误的答案。\n\n### 4.4 生成推理阶段-设计解码策略\n\n在事实性方面，核采样(又称顶点采样)不如贪婪解码，可归因于top-p采样为提高多样性而引入的随机性，这可能会无意中导致幻觉，因为LLMs往往会编造信息以产生多样化的反应。\n\n1. **引入事实核采样的解码算法**，旨在利用top-p和贪婪解码的优势，在多样性和事实性之间取得更有效的平衡。\n2. \"**验证链\" (Chain-of-Verification, COVE )的解码框架**。该框架基于这样的观察：独立的验证问题通常比长形式答案中提出的问题产生更准确的事实。COVE框架首先规划验证问题，然后回答这些问题，最终产生一个强化的、修正的回应。在多种任务上的实验结果表明，COVE能够有效缓解幻觉。\n3. **引入推理-时间干预(ITI)方法**，以提高LLM的真实性。该方法基于这样一个假设，即LLMs拥有与真实性相关的潜在的、可解释的子结构。ITI方法包括两个步骤:\n   1. 在LLM的每个注意头之上拟合一个二元分类器，以确定一组在回答事实性问题时具有卓越线性探测准确性的注意头;\n   2. 在推理过程中沿着这些与事实性相关的方向移动模型激活。\n4. **检索增强设置**\n   1. LLMs在处理下游任务时有时无法充分关注检索到的知识，尤其是当检索到的知识与LLMs的参数知识相冲突时。为了解决这个问题，可以采用直接的上下文感知解码(**CAD**)策略。\n   2. **CAD**方法旨在迫使LLM更多地关注上下文信息，而不是过度依赖自身的参数知识来做出决策。实验结果表明，CAD能有效激发LLM利用检索知识的能力，从而减少下游任务中的事实幻觉\n5. **修改模型结构**\n   1. 多分支解码器和不确定性感知解码器。在构建LLM时采用双向自回归架构，从而实现从左到右和从右到左的语言建模，这种设计策略可以有效地利用双向自回归结构，有效地利用双向信息，有助于减少幻觉。\n\n### 4.5 生成推理阶段-借助外部知识\n\n使用外部知识作为补充证据来帮助LLMs提供真实的回复，是最近兴起的一种解决方案。这种方法通常包括两个步骤:第一步是准确获取与用户指令相关的知识。一旦获得了有用的知识，第二步就需要利用这些知识来指导应答的生成。\n\n1. **知识获取**\n   1. 外部知识库：大规模非结构化语料库、结构化数据库、维基百科等特定网站，甚至整个互联网\n   2. 外部工具：FacTool针对特定的下游任务，利用不同的工具帮助检测LLM中的幻觉，如用于基于知识的质量保证的搜索引擎API、用于代码生成的代码执行器和用于科学文献审查的谷歌学术API。\n   3. CRITIC让LLM与多个工具交互，并自动修改其响应，这已被证明能有效提高真实性。\n2. **知识利用**\n   1. **生成时补充**\n      1. 利用检索到的知识或工具反馈的直接方法是，在提示LLMs之前直接将它们与用户查询串联起来，这种方法既有效又易于实施，这种知识也被称为内文知识。\n   2. **事后纠正**\n      1. 即在后处理阶段构建一个辅助固定器来纠正幻觉。固定器可以是另一个LLM，也可以是一个特定的小模型。这种固定器首先与外部知识源互动，收集足够的知识，然后纠正幻觉。\n\n### 4.6 生成推理阶段-利用不确定性\n\n不确定性是推理过程中保护和减少幻觉的重要指标。通常，它指的是模型结果的置信度。不确定性可以帮助用户确定何时信任LLM。只要能准确描述LLM响应的不确定性，用户就能过滤或纠正LLM的高不确定性声明，因为这类声明更容易是捏造的。\n\n![](image%205.png)\n\n1. **基于Logit的估计**\n\n   1. 这是一种基于对数的方法，它需要获取模型的logit，通常通过计算token级概率或熵来确定不确定性。\n\n2. **其次是基于口头估计**\n\n   1. 直接要求LLM表达其不确定度，例如使用以下提示:\"请回答并提供您的置信度分数(从0到100)\"。这种方法之所以有效，是因为当地语言学家的语言表达能力和服从指令的能力很强。也可以使用思维链提示来加强这种方法。\n\n3. **基于一致性估计**\n\n   1. 这种方法基于这样一个假设:当LLMs犹豫不决并对事实产生幻觉时，他们很可能会对同一问题做出逻辑上不一致的回答。\n   2. 使用BERTScore、基于QA的指标和n-gram指标进行计算，并将这些方法结合起来能产生最佳结果。直接利用额外的LLM来判断两个LLM反应在相同语境下是否存在逻辑矛盾，可以采用另一种LLM来修正两个反应中这种自相矛盾的幻觉。\n\n4. **多agent互动**\n\n   ![](image%206.png)\n\n多个LLM(也称为代理)独立提出建议，并就各自的回应进行协作辩论，以达成单一共识。\n\n### 4.3 个人总结\n\n缓解大模型幻觉业务中相较可行的措施：\n\n1. **构建高质量数据集：**\n   1. 利用模型筛选出可能导致幻觉的数据并筛除。\n   2. 预训练或SFT阶段给忠实度更高的数据加权，或者只使用可靠来源的数据（例如经过人工筛查的数据，医典、教科书等）。\n2. **优化模型结构：**\n   1. 检索增强被证明可以显著减少幻觉问题。\n   2. 在解码时减少模型的随机性（例如，业务中已经采用的降低temperture的方法）。因为divisersity和faithfulness是一个trade-off的关系，减少diversity或randomness可以变相提高faithfulness/factuality。\n   3. 设计更能充分编码以利用source information的方法。例如GNN网络，融入一些人类偏置。\n3. **优化训练方式：**\n   1. **多任务学习**，通过设计合适的额外任务，可以达到减轻幻觉的效果。\n   2. **后处理**，设计一个小模型，专门处理幻觉的问题。\n   3. **提前规划骨架**，再生成（sketch to content）。\n   4. **~~可控文本生成~~**，将幻觉视为一种额外的属性，利用可控文本生成技术进行控制。\n   5. **~~强化学习**，~~现有工作将减轻模型的幻觉作为reward函数，从而减轻幻觉现象。\n\n## 参考文献\n\n1. [Survey of Hallucination in Natural Language Generation](https://arxiv.org/abs/2202.03629)\n2. [Reading list of hallucination in LLMs](https://github.com/HillZhang1999/llm-hallucination-survey)\n3. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499 (2021).\n4. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. arXiv preprint arXiv:2206.04624 (2022).\n5. Chaojun Wang and Rico Sennrich. 2020. On exposure bias, hallucination and domain shift in neural machine translation. In Proceedings of the 2020 Annual Conference of the Association for Computational Linguistics. 3544–3552.\n6. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP’21). 7052–7063.\n7. Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’21). 4812–4829\n8. Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the ACL. 5055–5070.\n9. Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2021. Evaluating groundedness in dialogue systems: The BEGIN benchmark. In Findings of the Association for Computational Linguistics. Association for Computational Linguistics, 1–12.\n10. Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 3592–3603.\n11. Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of Wikipedia: Knowledge-powered conversational agents. In Proceedings of the International Conference on Learning Representations.\n12. Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. 2021. GO FIGURE: A meta evaluation of factuality in summarization. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, 478–487.\n13. Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. Q2: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 7856–7870\n14. Vectara：让你的LLM应用告别幻觉！：[https://zhuanlan.zhihu.com/p/626544154](https://zhuanlan.zhihu.com/p/626544154)\n15. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. arXiv preprint arXiv:2206.04624 (2022).\n16. Peng, Baolin, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang et al. \"Check your facts and try again: Improving large language models with external knowledge and automated feedback.\"arXiv preprint arXiv:2302.12813(2023).\n17. annah Rashkin, David Reitter, Gaurav Singh Tomar, and Dipanjan Das. 2021. Increasing faithfulness in knowledgegrounded dialogue with controllable features. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. 704–718.\n18. Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang, Xiang Gao, Chris Quirk, Rik Koncel-Kedziorski, et al. 2021. A controllable model of grounded response generation. In Proceedings of the AAAI Conference on Artificial Intelligence. 14085–14093.\n19. Ratish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-text generation with content selection and planning. In Proceedings of the AAAI Conference on Artificial Intelligence\n20. Yangming Li, Kaisheng Yao, Libo Qin, Wanxiang Che, Xiaolong Li, and Ting Liu. 2020. Slot-consistent NLG for task-oriented dialogue systems with iterative rectification network. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 97–106.\n21. Mohsen Mesgar, Edwin Simpson, and Iryna Gurevych. 2021. Improving factual consistency between a response and persona facts. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics. 549–562.\n22. Sihao Chen, Fan Zhang, Kazoo Sone, and Dan Roth. 2021. Improving faithfulness in abstractive summarization with contrast candidate generation and selection. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’21). 5935–5941.\n23. 大模型的幻觉问题调研: LLM Hallucination Survey: [https://zhuanlan.zhihu.com/p/642648601](\n","source":"_posts/LlmIllusion.md","raw":"---\ntitle: 大模型幻觉\ntags:\n- 大模型\ncategories:\n- 大模型相关\n---\n# 大模型幻觉\n\n## 一、什么是 大模型幻觉问题？\n\n### 1.1 大模型幻觉问题定义\n\n- 定义：当模型生成的**文本不遵循原文（Faithfulness）或者不符合事实（Factualness）**，我们就可以认为模型出现了幻觉的问题。\n\n### 1.2 何为 Faithfulness and Factualness？\n\n- Faithfulness：是否遵循input content；\n- Factualness：是否符合世界知识；\n\n![](image.png)\n\n（参考：Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models，[https://arxiv.org/abs/2309.01219](https://arxiv.org/abs/2309.01219)）\n\n![](image1.png)\n\n1. **输入相冲突的幻觉。**输入相冲突的幻觉，即LLM生成的内容与用户提供的源输入相背离。当LLM生成的内容偏离用户输入时，就会产生这类幻觉。\n2. **语境冲突性幻觉。**LLM生成的内容与之前生成的信息本身相冲突。LLMs在生成冗长或多轮回答时可能会表现出自我矛盾。这种类型的幻觉产生于LLMs在整个对话过程中失去对上下文的跟踪或无法保持一致性时，这可能是由于他们在保持长期记忆或识别相关上下文方面的局限性造成的。\n3. **与事实相冲突的幻觉。**与事实相冲突的幻觉指的是LLM生成的内容不忠实于既定的世界知识。事实冲突型幻觉。当LLM生成的信息或文本与已有的世界知识相矛盾时，就会出现这种类型的幻觉。如图2所示，事实冲突幻觉的来源可能多种多样，并在LLM生命周期的不同阶段出现。\n\n### 除幻觉外的常见问题:\n\n![](image2.png)\n\n1. **歧义性：**答案可能并不一定是不正确的，但是对于用户问题却没有给出一个有用的答案。\n2. **不完整性：**不完整性问题发生在生成的响应不完整或碎片化的情况下，如上图例子中只给出了换轮胎4个步骤中的2步。\n3. **偏见：**LLMs中的偏见是指生成文本中不公平或偏见态度的表现，这些偏见可能来源于训练数据。如上图例子中将教师描述为女性是一种偏见。\n4. **信息不足：**指LLMs逃避回答某些问题的倾向，即使其有能力这样做。例如，由于奖励模型的不完善，RLHF可能会导致LLMs的过度优化，从而可能导致模型输出信息不足。\n\n### 1.3 传统任务中的模型幻觉 vs LLMs 中模型幻觉\n\n1. **在传统任务里，幻觉大都是指的是Faithfulness**：\n   1. **Intrinsic Hallucination（信息冲突）**: LMs在生成回复时，与输入信息产生了冲突，例如摘要问题里，abstract和document的信息不一致；\n   2. **Extrinsic Hallucination（无中生有）**: LMs在生成回复时，输出一些并没有体现在输入中的额外信息，比如邮箱地址、电话号码、住址，并且难以验证其真假。（PS: 按照此定义，Extrinsic Hallucination有可能是真的信息，只是需要外部信息源进行认证）\n2. **而面向LLMs，我们通常考虑的幻觉则是Factualness**：\n   1. 因为我们应用LLM的形式是open-domain Chat，而不是局限于特定任务，所以数据源可以看做任意的世界知识。LLMs如果生成了不在input source里的额外信息，但是符合事实的，这种情况也可能是对我们有帮助的。\n\n## 二、大模型幻觉从何而来？\n\n### 2.1 从 数据角度 进行分析\n\n在 数据构建过程中，由于以下问题，导致 模型幻觉 的 发生：\n\n1. 训练数据可信度问题。由于 大模型 的 训练数据 都是 通过 众包/爬虫检索 方式 收集得到的，这种数据构建方式的优点是量比较大，但是缺点是 包含 大量虚假信息。这种虚假信息 直接导致的问题就是使 模型出现错误认知；\n2. 重复数据问题。过多的重复信息也可能导致模型的知识记忆出现bias，从而导致幻觉；\n3. LLM偏向于肯定测试的样本，即人类生成的语料中也存在幻觉(可反映为过时的、双重的或捏造的表达)，LLMs很容易复制甚至放大这种幻觉行为。\n\n> 引用至 [3] Deduplicating training data makes language models better\n\n### 2.2 从 模型角度 进行分析\n\n不止是 数据角度问题，大模型幻觉问题 出现的原因 还 表现在 模型角度。\n\n- **模型结构**：如果是较弱的backbone（比如RNN）可能导致比较严重的幻觉问题，但在LLMs时代应该不太可能存在这一问题；\n- **解码算法**：\n  - 研究表明，**如果使用不确定性较高的采样算法（e.g.，top-p）会诱导LMs出现更严重的幻觉问题**。甚至可以故意在解码算法中加入一些随机性，进一步让LMs胡编乱造（可以用该方法生成一些negative samples）\n  - 局部最优化(标记预测)并不一定能确保全局最优化(序列预测)，早期的局部预测可能会将LLMs带入难以形成正确反应的境地。\n\n引用至 [4] Factuality enhanced language models for open-ended text generation\n\n- **暴露偏差**：**训练和测试阶段不匹配的exposure bias问题可能导致LLMs出现幻觉，特别是生成long-form response的时候**。\n\n> 引用至 [5] On exposure bias, hallucination and domain shift in neural machine translation\n\n- **参数知识**：**LMs在预训练阶段记忆的错误的知识，将会严重导致幻觉问题**。\n\n> 引用至 [6] Entity-based knowledge conflicts in question answering\n\n## 三、如何 评估 大模型幻觉问题？\n\n**现有的传统幻觉评估指标和人类结果的相关性往往较低**，同时大都是task-specific的 [7]。\n\n### 3.1 Reference-based\n\nReference-based的指标有两类：\n\n1. **基于Source Information和Target Reference**：利用一些统计学指标，比如**ROUGE、BLEU来评估输出结果和Source/Target信息的重叠度**;\n2. **基于Source Information**：由于NLG任务里，Target输出往往是多种多样的，因此许多工作**只基于Source信息进行幻觉的评估**。比如Knowledge F1。\n\n基于Reference的评价指标，**基本上只能评价Faithfulness，而无法评价Factualness，因此通常不适用于LLMs**。\n\n### 3.2 Reference-Free\n\n### 3.2.1 基于IE\n\n- 介绍：**将知识限定于可以用三元组形式表示的关系和事件**，基于额外的IE模型进行抽取，接着使用额外模型进行验证；\n- 缺点：\n  - 可能存在IE模型的错误传播问题；\n  - 知识被限定在三元组形式。\n\n### 3.2.2 基于QA\n\n- 介绍：\n\n1. 第一步先**基于LM生成的回复**，使用一个QG(question generation)模型生成一系列QA pairs；\n2. 第二步**给定Source Information，让QA模型对上一步生成的Question进行回复**；\n3. 第三步则是**通过对比第一步的answers和第二步的answers，计算匹配指标，衡量模型的幻觉问题**；\n\n- 缺点：可能存在IE模型的错误传播问题；难以评估Factualness，因为上述第二步里面，Source Information不可能包含全部的世界知识，因此对于一些问题难以生成可靠的回复。\n\n> 引用至 [8] FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization\n\n### 3.2.3 基于NLI\n\n- 介绍：基于NLI的方法**通过NLI模型评估是否Source Information可以蕴含Generated Text，从而评估是否出现了幻觉现象**。\n  - 缺点：\n    - Off-the-shelf NLI模型用于核查事实效果不是很好\n\n> 引用至 [9] Evaluating groundedness in dialogue systems: The BEGIN benchmark.\n\n- 无法评估需要世界知识的幻觉问题：\n  - **仅能依赖于Source进行核查**；\n  - 都是sentence-level的，**无法支撑更细粒度的幻觉检查；**\n\n> 引用至 [10] Evaluating factuality in generation with dependency-level entailment.\n\n- 幻觉问题和蕴含问题实际并不等价：\n\n1. 例子：Putin is president. -> Putin is U.S. president (可以蕴含，但是是幻觉)\n\n### 3.2.4 基于Factualness Classification Metric\n\n- 介绍：标注/构造一批和幻觉/事实有关的数据，训练检测模型，利用该模型评估新生成文本的幻觉/事实问题。\n\n> 引用至 [11] Knowledge-powered conversational agents\n\n### 3.2.5 人工评估\n\n- 介绍：目前为止最靠谱的，此外还可以依靠LLM打分（比如利用GPT4，但是GPT4也存在着严重的幻觉问题，即使经过retrival-augment，检索回来的信息也有可能是错误的）\n\n## 四、如何 缓解 大模型幻觉问题？\n\n### 4.1预训练阶段\n\nLLM的知识大多是在预训练阶段获得的。在预训练语料库中存在诸如错误信息之类的噪声数据可能会破坏LLMs的参数知识，而这是导致误差的一个重要因素。有时，还可以将语言模型获得的事实知识追溯到其训练数据。**因此，减少幻觉的直观方法可以是人工或自动整理预训练语料库，尽可能减少无法验证或不可靠的数据。**\n\n1. **人工消除噪声训练数据**\n   1. 专注于\"数据到文本\"(data-to-text)任务，并邀请人工根据给定的知识库手动编写干净准确的回复。\n   2. 在现有的表格到文本数据集中对文本进行人工提炼这一过程也大大减少了事实幻觉。\n   3. 在构建表对文训练数据时，指导注释者修改维基百科中已验证的句子，而不是直接创建新句子。\n2. **自动选择可靠数据或过滤掉噪声数据**\n   1. **GPT-3**的预训练数据就是通过与一系列高质量参考数据的相似性进行清理。\n   2. **Falcon**通过启发式规则从网络中仔细提取高质量数据，并证明了经过适当分级的相关语料库可以产生强大的LLM。为了减少幻觉，目前的LLM通常会从可靠的文本来源收集预训练数据。\n   3. **Llama2**在构建预训练语料库时，从维基百科等高度事实性的来源中向上抽取数据\n   4. 在事实性文档的句子前加上主题前缀，使每个句子在预训练时都能作为一个独立的事实，将文档名称作为主题前缀，并观察到这种方法提高了LLM在**TruthfulQA**上的性能**。**\n\n### 4.2 SFT阶段\n\n![](image%203.png)\n\n当前的LLMs一般都会经历一个被称为\"监督微调\"(SFT)的过程，以从预训练中获取所需的知识，并学习如何与用户互动。SFT通常包括首先注释或收集海量任务指令跟踪数据，然后使用极大似然法(MLE)在这些数据上对预先训练的基础LLM进行微调。\n\n**目的**\n\n- 学习如何与用户进行有效交互。\n- 从预训练中获得的知识中抽取信息。\n\n1. **加工整理数据**\n   1. 与预训练类似，一种可能的方法是要减少SFT阶段的幻觉，可以对训练数据进行整理，SFT数据量相对较小。\n   2. 手动和自动整理都是可行的选择。在未经编辑的数据上进行微调的LLM相比，在这些经过编辑的指令数据上进行微调的LLM表现出更高的真实性和事实性水平。\n2. **加入拒答**\n   1. 在推理过程中，如果被要求回答与未学知识相关的问题，他们很可能会自信地产生幻觉。\n   2. 采用以诚实为导向的SFT，即在SFT数据中引入一些诚实样本。诚实样本指的是承认自己无能的回答，如\"对不起，我不知道\"。学会拒绝回答特定问题，从而帮助减少幻觉。\n\n### 4.3 RLHF期间的缓解\n\n![](image%204.png)\n\n1. 利用人类反馈不仅能缩小机器生成的内容与人类偏好之间的差距，还能帮助LLM符合预期的标准或目标。目前常用的一个标准是\"3H\"，即有益、诚实和无害。这里的\"诚实\"只是指在LLM反应中尽量减少幻觉。\n2. 强化学习可以引导LLM探索其知识边界，使他们能够拒绝回答超出其能力范围的问题，而不是编造不真实的回答。但这种方法也带来了独特的挑战。例如，由于在乐于助人和诚实之间的权衡失衡，经过RL调整的LLM可能会表现出过度保守。\n3. **GPT4使用合成幻觉数据来训练奖励模型并执行R**L，从而将TruthfulQA的准确率从约30%提高到60%。\n4. 设计一种专门用于减轻幻觉的特殊奖励函数:\"Unhedged/Hedged Correct/Wrong\"，指的是LLM用肯定或犹豫的语气提供正确或错误的答案。\n\n### 4.4 生成推理阶段-设计解码策略\n\n在事实性方面，核采样(又称顶点采样)不如贪婪解码，可归因于top-p采样为提高多样性而引入的随机性，这可能会无意中导致幻觉，因为LLMs往往会编造信息以产生多样化的反应。\n\n1. **引入事实核采样的解码算法**，旨在利用top-p和贪婪解码的优势，在多样性和事实性之间取得更有效的平衡。\n2. \"**验证链\" (Chain-of-Verification, COVE )的解码框架**。该框架基于这样的观察：独立的验证问题通常比长形式答案中提出的问题产生更准确的事实。COVE框架首先规划验证问题，然后回答这些问题，最终产生一个强化的、修正的回应。在多种任务上的实验结果表明，COVE能够有效缓解幻觉。\n3. **引入推理-时间干预(ITI)方法**，以提高LLM的真实性。该方法基于这样一个假设，即LLMs拥有与真实性相关的潜在的、可解释的子结构。ITI方法包括两个步骤:\n   1. 在LLM的每个注意头之上拟合一个二元分类器，以确定一组在回答事实性问题时具有卓越线性探测准确性的注意头;\n   2. 在推理过程中沿着这些与事实性相关的方向移动模型激活。\n4. **检索增强设置**\n   1. LLMs在处理下游任务时有时无法充分关注检索到的知识，尤其是当检索到的知识与LLMs的参数知识相冲突时。为了解决这个问题，可以采用直接的上下文感知解码(**CAD**)策略。\n   2. **CAD**方法旨在迫使LLM更多地关注上下文信息，而不是过度依赖自身的参数知识来做出决策。实验结果表明，CAD能有效激发LLM利用检索知识的能力，从而减少下游任务中的事实幻觉\n5. **修改模型结构**\n   1. 多分支解码器和不确定性感知解码器。在构建LLM时采用双向自回归架构，从而实现从左到右和从右到左的语言建模，这种设计策略可以有效地利用双向自回归结构，有效地利用双向信息，有助于减少幻觉。\n\n### 4.5 生成推理阶段-借助外部知识\n\n使用外部知识作为补充证据来帮助LLMs提供真实的回复，是最近兴起的一种解决方案。这种方法通常包括两个步骤:第一步是准确获取与用户指令相关的知识。一旦获得了有用的知识，第二步就需要利用这些知识来指导应答的生成。\n\n1. **知识获取**\n   1. 外部知识库：大规模非结构化语料库、结构化数据库、维基百科等特定网站，甚至整个互联网\n   2. 外部工具：FacTool针对特定的下游任务，利用不同的工具帮助检测LLM中的幻觉，如用于基于知识的质量保证的搜索引擎API、用于代码生成的代码执行器和用于科学文献审查的谷歌学术API。\n   3. CRITIC让LLM与多个工具交互，并自动修改其响应，这已被证明能有效提高真实性。\n2. **知识利用**\n   1. **生成时补充**\n      1. 利用检索到的知识或工具反馈的直接方法是，在提示LLMs之前直接将它们与用户查询串联起来，这种方法既有效又易于实施，这种知识也被称为内文知识。\n   2. **事后纠正**\n      1. 即在后处理阶段构建一个辅助固定器来纠正幻觉。固定器可以是另一个LLM，也可以是一个特定的小模型。这种固定器首先与外部知识源互动，收集足够的知识，然后纠正幻觉。\n\n### 4.6 生成推理阶段-利用不确定性\n\n不确定性是推理过程中保护和减少幻觉的重要指标。通常，它指的是模型结果的置信度。不确定性可以帮助用户确定何时信任LLM。只要能准确描述LLM响应的不确定性，用户就能过滤或纠正LLM的高不确定性声明，因为这类声明更容易是捏造的。\n\n![](image%205.png)\n\n1. **基于Logit的估计**\n\n   1. 这是一种基于对数的方法，它需要获取模型的logit，通常通过计算token级概率或熵来确定不确定性。\n\n2. **其次是基于口头估计**\n\n   1. 直接要求LLM表达其不确定度，例如使用以下提示:\"请回答并提供您的置信度分数(从0到100)\"。这种方法之所以有效，是因为当地语言学家的语言表达能力和服从指令的能力很强。也可以使用思维链提示来加强这种方法。\n\n3. **基于一致性估计**\n\n   1. 这种方法基于这样一个假设:当LLMs犹豫不决并对事实产生幻觉时，他们很可能会对同一问题做出逻辑上不一致的回答。\n   2. 使用BERTScore、基于QA的指标和n-gram指标进行计算，并将这些方法结合起来能产生最佳结果。直接利用额外的LLM来判断两个LLM反应在相同语境下是否存在逻辑矛盾，可以采用另一种LLM来修正两个反应中这种自相矛盾的幻觉。\n\n4. **多agent互动**\n\n   ![](image%206.png)\n\n多个LLM(也称为代理)独立提出建议，并就各自的回应进行协作辩论，以达成单一共识。\n\n### 4.3 个人总结\n\n缓解大模型幻觉业务中相较可行的措施：\n\n1. **构建高质量数据集：**\n   1. 利用模型筛选出可能导致幻觉的数据并筛除。\n   2. 预训练或SFT阶段给忠实度更高的数据加权，或者只使用可靠来源的数据（例如经过人工筛查的数据，医典、教科书等）。\n2. **优化模型结构：**\n   1. 检索增强被证明可以显著减少幻觉问题。\n   2. 在解码时减少模型的随机性（例如，业务中已经采用的降低temperture的方法）。因为divisersity和faithfulness是一个trade-off的关系，减少diversity或randomness可以变相提高faithfulness/factuality。\n   3. 设计更能充分编码以利用source information的方法。例如GNN网络，融入一些人类偏置。\n3. **优化训练方式：**\n   1. **多任务学习**，通过设计合适的额外任务，可以达到减轻幻觉的效果。\n   2. **后处理**，设计一个小模型，专门处理幻觉的问题。\n   3. **提前规划骨架**，再生成（sketch to content）。\n   4. **~~可控文本生成~~**，将幻觉视为一种额外的属性，利用可控文本生成技术进行控制。\n   5. **~~强化学习**，~~现有工作将减轻模型的幻觉作为reward函数，从而减轻幻觉现象。\n\n## 参考文献\n\n1. [Survey of Hallucination in Natural Language Generation](https://arxiv.org/abs/2202.03629)\n2. [Reading list of hallucination in LLMs](https://github.com/HillZhang1999/llm-hallucination-survey)\n3. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499 (2021).\n4. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. arXiv preprint arXiv:2206.04624 (2022).\n5. Chaojun Wang and Rico Sennrich. 2020. On exposure bias, hallucination and domain shift in neural machine translation. In Proceedings of the 2020 Annual Conference of the Association for Computational Linguistics. 3544–3552.\n6. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP’21). 7052–7063.\n7. Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’21). 4812–4829\n8. Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the ACL. 5055–5070.\n9. Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2021. Evaluating groundedness in dialogue systems: The BEGIN benchmark. In Findings of the Association for Computational Linguistics. Association for Computational Linguistics, 1–12.\n10. Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 3592–3603.\n11. Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of Wikipedia: Knowledge-powered conversational agents. In Proceedings of the International Conference on Learning Representations.\n12. Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. 2021. GO FIGURE: A meta evaluation of factuality in summarization. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, 478–487.\n13. Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. Q2: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 7856–7870\n14. Vectara：让你的LLM应用告别幻觉！：[https://zhuanlan.zhihu.com/p/626544154](https://zhuanlan.zhihu.com/p/626544154)\n15. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. arXiv preprint arXiv:2206.04624 (2022).\n16. Peng, Baolin, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang et al. \"Check your facts and try again: Improving large language models with external knowledge and automated feedback.\"arXiv preprint arXiv:2302.12813(2023).\n17. annah Rashkin, David Reitter, Gaurav Singh Tomar, and Dipanjan Das. 2021. Increasing faithfulness in knowledgegrounded dialogue with controllable features. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. 704–718.\n18. Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang, Xiang Gao, Chris Quirk, Rik Koncel-Kedziorski, et al. 2021. A controllable model of grounded response generation. In Proceedings of the AAAI Conference on Artificial Intelligence. 14085–14093.\n19. Ratish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-text generation with content selection and planning. In Proceedings of the AAAI Conference on Artificial Intelligence\n20. Yangming Li, Kaisheng Yao, Libo Qin, Wanxiang Che, Xiaolong Li, and Ting Liu. 2020. Slot-consistent NLG for task-oriented dialogue systems with iterative rectification network. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 97–106.\n21. Mohsen Mesgar, Edwin Simpson, and Iryna Gurevych. 2021. Improving factual consistency between a response and persona facts. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics. 549–562.\n22. Sihao Chen, Fan Zhang, Kazoo Sone, and Dan Roth. 2021. Improving faithfulness in abstractive summarization with contrast candidate generation and selection. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’21). 5935–5941.\n23. 大模型的幻觉问题调研: LLM Hallucination Survey: [https://zhuanlan.zhihu.com/p/642648601](\n","slug":"LlmIllusion","published":1,"date":"2024-09-01T12:40:17.980Z","updated":"2024-09-01T14:53:47.932Z","_id":"cm0jk8pjy00028uiofoxk54dm","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"大模型幻觉\"><a href=\"#大模型幻觉\" class=\"headerlink\" title=\"大模型幻觉\"></a>大模型幻觉</h1><h2 id=\"一、什么是-大模型幻觉问题？\"><a href=\"#一、什么是-大模型幻觉问题？\" class=\"headerlink\" title=\"一、什么是 大模型幻觉问题？\"></a>一、什么是 大模型幻觉问题？</h2><h3 id=\"1-1-大模型幻觉问题定义\"><a href=\"#1-1-大模型幻觉问题定义\" class=\"headerlink\" title=\"1.1 大模型幻觉问题定义\"></a>1.1 大模型幻觉问题定义</h3><ul>\n<li>定义：当模型生成的<strong>文本不遵循原文（Faithfulness）或者不符合事实（Factualness）</strong>，我们就可以认为模型出现了幻觉的问题。</li>\n</ul>\n<h3 id=\"1-2-何为-Faithfulness-and-Factualness？\"><a href=\"#1-2-何为-Faithfulness-and-Factualness？\" class=\"headerlink\" title=\"1.2 何为 Faithfulness and Factualness？\"></a>1.2 何为 Faithfulness and Factualness？</h3><ul>\n<li>Faithfulness：是否遵循input content；</li>\n<li>Factualness：是否符合世界知识；</li>\n</ul>\n<p><img src=\"/2024/09/01/hello-world/image.png\"></p>\n<p>（参考：Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models，<a href=\"https://arxiv.org/abs/2309.01219\">https://arxiv.org/abs/2309.01219</a>）</p>\n<p><img src=\"/2024/09/01/hello-world/image1.png\"></p>\n<ol>\n<li><strong>输入相冲突的幻觉。</strong>输入相冲突的幻觉，即LLM生成的内容与用户提供的源输入相背离。当LLM生成的内容偏离用户输入时，就会产生这类幻觉。</li>\n<li><strong>语境冲突性幻觉。</strong>LLM生成的内容与之前生成的信息本身相冲突。LLMs在生成冗长或多轮回答时可能会表现出自我矛盾。这种类型的幻觉产生于LLMs在整个对话过程中失去对上下文的跟踪或无法保持一致性时，这可能是由于他们在保持长期记忆或识别相关上下文方面的局限性造成的。</li>\n<li><strong>与事实相冲突的幻觉。</strong>与事实相冲突的幻觉指的是LLM生成的内容不忠实于既定的世界知识。事实冲突型幻觉。当LLM生成的信息或文本与已有的世界知识相矛盾时，就会出现这种类型的幻觉。如图2所示，事实冲突幻觉的来源可能多种多样，并在LLM生命周期的不同阶段出现。</li>\n</ol>\n<h3 id=\"除幻觉外的常见问题\"><a href=\"#除幻觉外的常见问题\" class=\"headerlink\" title=\"除幻觉外的常见问题:\"></a>除幻觉外的常见问题:</h3><p><img src=\"/2024/09/01/hello-world/image2.png\"></p>\n<ol>\n<li><strong>歧义性：</strong>答案可能并不一定是不正确的，但是对于用户问题却没有给出一个有用的答案。</li>\n<li><strong>不完整性：</strong>不完整性问题发生在生成的响应不完整或碎片化的情况下，如上图例子中只给出了换轮胎4个步骤中的2步。</li>\n<li><strong>偏见：</strong>LLMs中的偏见是指生成文本中不公平或偏见态度的表现，这些偏见可能来源于训练数据。如上图例子中将教师描述为女性是一种偏见。</li>\n<li><strong>信息不足：</strong>指LLMs逃避回答某些问题的倾向，即使其有能力这样做。例如，由于奖励模型的不完善，RLHF可能会导致LLMs的过度优化，从而可能导致模型输出信息不足。</li>\n</ol>\n<h3 id=\"1-3-传统任务中的模型幻觉-vs-LLMs-中模型幻觉\"><a href=\"#1-3-传统任务中的模型幻觉-vs-LLMs-中模型幻觉\" class=\"headerlink\" title=\"1.3 传统任务中的模型幻觉 vs LLMs 中模型幻觉\"></a>1.3 传统任务中的模型幻觉 vs LLMs 中模型幻觉</h3><ol>\n<li><strong>在传统任务里，幻觉大都是指的是Faithfulness</strong>：<ol>\n<li><strong>Intrinsic Hallucination（信息冲突）</strong>: LMs在生成回复时，与输入信息产生了冲突，例如摘要问题里，abstract和document的信息不一致；</li>\n<li><strong>Extrinsic Hallucination（无中生有）</strong>: LMs在生成回复时，输出一些并没有体现在输入中的额外信息，比如邮箱地址、电话号码、住址，并且难以验证其真假。（PS: 按照此定义，Extrinsic Hallucination有可能是真的信息，只是需要外部信息源进行认证）</li>\n</ol>\n</li>\n<li><strong>而面向LLMs，我们通常考虑的幻觉则是Factualness</strong>：<ol>\n<li>因为我们应用LLM的形式是open-domain Chat，而不是局限于特定任务，所以数据源可以看做任意的世界知识。LLMs如果生成了不在input source里的额外信息，但是符合事实的，这种情况也可能是对我们有帮助的。</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"二、大模型幻觉从何而来？\"><a href=\"#二、大模型幻觉从何而来？\" class=\"headerlink\" title=\"二、大模型幻觉从何而来？\"></a>二、大模型幻觉从何而来？</h2><h3 id=\"2-1-从-数据角度-进行分析\"><a href=\"#2-1-从-数据角度-进行分析\" class=\"headerlink\" title=\"2.1 从 数据角度 进行分析\"></a>2.1 从 数据角度 进行分析</h3><p>在 数据构建过程中，由于以下问题，导致 模型幻觉 的 发生：</p>\n<ol>\n<li>训练数据可信度问题。由于 大模型 的 训练数据 都是 通过 众包&#x2F;爬虫检索 方式 收集得到的，这种数据构建方式的优点是量比较大，但是缺点是 包含 大量虚假信息。这种虚假信息 直接导致的问题就是使 模型出现错误认知；</li>\n<li>重复数据问题。过多的重复信息也可能导致模型的知识记忆出现bias，从而导致幻觉；</li>\n<li>LLM偏向于肯定测试的样本，即人类生成的语料中也存在幻觉(可反映为过时的、双重的或捏造的表达)，LLMs很容易复制甚至放大这种幻觉行为。</li>\n</ol>\n<blockquote>\n<p>引用至 [3] Deduplicating training data makes language models better</p>\n</blockquote>\n<h3 id=\"2-2-从-模型角度-进行分析\"><a href=\"#2-2-从-模型角度-进行分析\" class=\"headerlink\" title=\"2.2 从 模型角度 进行分析\"></a>2.2 从 模型角度 进行分析</h3><p>不止是 数据角度问题，大模型幻觉问题 出现的原因 还 表现在 模型角度。</p>\n<ul>\n<li><strong>模型结构</strong>：如果是较弱的backbone（比如RNN）可能导致比较严重的幻觉问题，但在LLMs时代应该不太可能存在这一问题；</li>\n<li><strong>解码算法</strong>：<ul>\n<li>研究表明，<strong>如果使用不确定性较高的采样算法（e.g.，top-p）会诱导LMs出现更严重的幻觉问题</strong>。甚至可以故意在解码算法中加入一些随机性，进一步让LMs胡编乱造（可以用该方法生成一些negative samples）</li>\n<li>局部最优化(标记预测)并不一定能确保全局最优化(序列预测)，早期的局部预测可能会将LLMs带入难以形成正确反应的境地。</li>\n</ul>\n</li>\n</ul>\n<p>引用至 [4] Factuality enhanced language models for open-ended text generation</p>\n<ul>\n<li><strong>暴露偏差</strong>：<strong>训练和测试阶段不匹配的exposure bias问题可能导致LLMs出现幻觉，特别是生成long-form response的时候</strong>。</li>\n</ul>\n<blockquote>\n<p>引用至 [5] On exposure bias, hallucination and domain shift in neural machine translation</p>\n</blockquote>\n<ul>\n<li><strong>参数知识</strong>：<strong>LMs在预训练阶段记忆的错误的知识，将会严重导致幻觉问题</strong>。</li>\n</ul>\n<blockquote>\n<p>引用至 [6] Entity-based knowledge conflicts in question answering</p>\n</blockquote>\n<h2 id=\"三、如何-评估-大模型幻觉问题？\"><a href=\"#三、如何-评估-大模型幻觉问题？\" class=\"headerlink\" title=\"三、如何 评估 大模型幻觉问题？\"></a>三、如何 评估 大模型幻觉问题？</h2><p><strong>现有的传统幻觉评估指标和人类结果的相关性往往较低</strong>，同时大都是task-specific的 [7]。</p>\n<h3 id=\"3-1-Reference-based\"><a href=\"#3-1-Reference-based\" class=\"headerlink\" title=\"3.1 Reference-based\"></a>3.1 Reference-based</h3><p>Reference-based的指标有两类：</p>\n<ol>\n<li><strong>基于Source Information和Target Reference</strong>：利用一些统计学指标，比如<strong>ROUGE、BLEU来评估输出结果和Source&#x2F;Target信息的重叠度</strong>;</li>\n<li><strong>基于Source Information</strong>：由于NLG任务里，Target输出往往是多种多样的，因此许多工作<strong>只基于Source信息进行幻觉的评估</strong>。比如Knowledge F1。</li>\n</ol>\n<p>基于Reference的评价指标，<strong>基本上只能评价Faithfulness，而无法评价Factualness，因此通常不适用于LLMs</strong>。</p>\n<h3 id=\"3-2-Reference-Free\"><a href=\"#3-2-Reference-Free\" class=\"headerlink\" title=\"3.2 Reference-Free\"></a>3.2 Reference-Free</h3><h3 id=\"3-2-1-基于IE\"><a href=\"#3-2-1-基于IE\" class=\"headerlink\" title=\"3.2.1 基于IE\"></a>3.2.1 基于IE</h3><ul>\n<li>介绍：<strong>将知识限定于可以用三元组形式表示的关系和事件</strong>，基于额外的IE模型进行抽取，接着使用额外模型进行验证；</li>\n<li>缺点：<ul>\n<li>可能存在IE模型的错误传播问题；</li>\n<li>知识被限定在三元组形式。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-2-2-基于QA\"><a href=\"#3-2-2-基于QA\" class=\"headerlink\" title=\"3.2.2 基于QA\"></a>3.2.2 基于QA</h3><ul>\n<li>介绍：</li>\n</ul>\n<ol>\n<li>第一步先<strong>基于LM生成的回复</strong>，使用一个QG(question generation)模型生成一系列QA pairs；</li>\n<li>第二步<strong>给定Source Information，让QA模型对上一步生成的Question进行回复</strong>；</li>\n<li>第三步则是<strong>通过对比第一步的answers和第二步的answers，计算匹配指标，衡量模型的幻觉问题</strong>；</li>\n</ol>\n<ul>\n<li>缺点：可能存在IE模型的错误传播问题；难以评估Factualness，因为上述第二步里面，Source Information不可能包含全部的世界知识，因此对于一些问题难以生成可靠的回复。</li>\n</ul>\n<blockquote>\n<p>引用至 [8] FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization</p>\n</blockquote>\n<h3 id=\"3-2-3-基于NLI\"><a href=\"#3-2-3-基于NLI\" class=\"headerlink\" title=\"3.2.3 基于NLI\"></a>3.2.3 基于NLI</h3><ul>\n<li>介绍：基于NLI的方法<strong>通过NLI模型评估是否Source Information可以蕴含Generated Text，从而评估是否出现了幻觉现象</strong>。<ul>\n<li>缺点：<ul>\n<li>Off-the-shelf NLI模型用于核查事实效果不是很好</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>引用至 [9] Evaluating groundedness in dialogue systems: The BEGIN benchmark.</p>\n</blockquote>\n<ul>\n<li>无法评估需要世界知识的幻觉问题：<ul>\n<li><strong>仅能依赖于Source进行核查</strong>；</li>\n<li>都是sentence-level的，<strong>无法支撑更细粒度的幻觉检查；</strong></li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>引用至 [10] Evaluating factuality in generation with dependency-level entailment.</p>\n</blockquote>\n<ul>\n<li>幻觉问题和蕴含问题实际并不等价：</li>\n</ul>\n<ol>\n<li>例子：Putin is president. -&gt; Putin is U.S. president (可以蕴含，但是是幻觉)</li>\n</ol>\n<h3 id=\"3-2-4-基于Factualness-Classification-Metric\"><a href=\"#3-2-4-基于Factualness-Classification-Metric\" class=\"headerlink\" title=\"3.2.4 基于Factualness Classification Metric\"></a>3.2.4 基于Factualness Classification Metric</h3><ul>\n<li>介绍：标注&#x2F;构造一批和幻觉&#x2F;事实有关的数据，训练检测模型，利用该模型评估新生成文本的幻觉&#x2F;事实问题。</li>\n</ul>\n<blockquote>\n<p>引用至 [11] Knowledge-powered conversational agents</p>\n</blockquote>\n<h3 id=\"3-2-5-人工评估\"><a href=\"#3-2-5-人工评估\" class=\"headerlink\" title=\"3.2.5 人工评估\"></a>3.2.5 人工评估</h3><ul>\n<li>介绍：目前为止最靠谱的，此外还可以依靠LLM打分（比如利用GPT4，但是GPT4也存在着严重的幻觉问题，即使经过retrival-augment，检索回来的信息也有可能是错误的）</li>\n</ul>\n<h2 id=\"四、如何-缓解-大模型幻觉问题？\"><a href=\"#四、如何-缓解-大模型幻觉问题？\" class=\"headerlink\" title=\"四、如何 缓解 大模型幻觉问题？\"></a>四、如何 缓解 大模型幻觉问题？</h2><h3 id=\"4-1预训练阶段\"><a href=\"#4-1预训练阶段\" class=\"headerlink\" title=\"4.1预训练阶段\"></a>4.1预训练阶段</h3><p>LLM的知识大多是在预训练阶段获得的。在预训练语料库中存在诸如错误信息之类的噪声数据可能会破坏LLMs的参数知识，而这是导致误差的一个重要因素。有时，还可以将语言模型获得的事实知识追溯到其训练数据。<strong>因此，减少幻觉的直观方法可以是人工或自动整理预训练语料库，尽可能减少无法验证或不可靠的数据。</strong></p>\n<ol>\n<li><strong>人工消除噪声训练数据</strong><ol>\n<li>专注于”数据到文本”(data-to-text)任务，并邀请人工根据给定的知识库手动编写干净准确的回复。</li>\n<li>在现有的表格到文本数据集中对文本进行人工提炼这一过程也大大减少了事实幻觉。</li>\n<li>在构建表对文训练数据时，指导注释者修改维基百科中已验证的句子，而不是直接创建新句子。</li>\n</ol>\n</li>\n<li><strong>自动选择可靠数据或过滤掉噪声数据</strong><ol>\n<li><strong>GPT-3</strong>的预训练数据就是通过与一系列高质量参考数据的相似性进行清理。</li>\n<li><strong>Falcon</strong>通过启发式规则从网络中仔细提取高质量数据，并证明了经过适当分级的相关语料库可以产生强大的LLM。为了减少幻觉，目前的LLM通常会从可靠的文本来源收集预训练数据。</li>\n<li><strong>Llama2</strong>在构建预训练语料库时，从维基百科等高度事实性的来源中向上抽取数据</li>\n<li>在事实性文档的句子前加上主题前缀，使每个句子在预训练时都能作为一个独立的事实，将文档名称作为主题前缀，并观察到这种方法提高了LLM在<strong>TruthfulQA</strong>上的性能<strong>。</strong></li>\n</ol>\n</li>\n</ol>\n<h3 id=\"4-2-SFT阶段\"><a href=\"#4-2-SFT阶段\" class=\"headerlink\" title=\"4.2 SFT阶段\"></a>4.2 SFT阶段</h3><p><img src=\"/image%203.png\"></p>\n<p>当前的LLMs一般都会经历一个被称为”监督微调”(SFT)的过程，以从预训练中获取所需的知识，并学习如何与用户互动。SFT通常包括首先注释或收集海量任务指令跟踪数据，然后使用极大似然法(MLE)在这些数据上对预先训练的基础LLM进行微调。</p>\n<p><strong>目的</strong></p>\n<ul>\n<li>学习如何与用户进行有效交互。</li>\n<li>从预训练中获得的知识中抽取信息。</li>\n</ul>\n<ol>\n<li><strong>加工整理数据</strong><ol>\n<li>与预训练类似，一种可能的方法是要减少SFT阶段的幻觉，可以对训练数据进行整理，SFT数据量相对较小。</li>\n<li>手动和自动整理都是可行的选择。在未经编辑的数据上进行微调的LLM相比，在这些经过编辑的指令数据上进行微调的LLM表现出更高的真实性和事实性水平。</li>\n</ol>\n</li>\n<li><strong>加入拒答</strong><ol>\n<li>在推理过程中，如果被要求回答与未学知识相关的问题，他们很可能会自信地产生幻觉。</li>\n<li>采用以诚实为导向的SFT，即在SFT数据中引入一些诚实样本。诚实样本指的是承认自己无能的回答，如”对不起，我不知道”。学会拒绝回答特定问题，从而帮助减少幻觉。</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"4-3-RLHF期间的缓解\"><a href=\"#4-3-RLHF期间的缓解\" class=\"headerlink\" title=\"4.3 RLHF期间的缓解\"></a>4.3 RLHF期间的缓解</h3><p><img src=\"/image%204.png\"></p>\n<ol>\n<li>利用人类反馈不仅能缩小机器生成的内容与人类偏好之间的差距，还能帮助LLM符合预期的标准或目标。目前常用的一个标准是”3H”，即有益、诚实和无害。这里的”诚实”只是指在LLM反应中尽量减少幻觉。</li>\n<li>强化学习可以引导LLM探索其知识边界，使他们能够拒绝回答超出其能力范围的问题，而不是编造不真实的回答。但这种方法也带来了独特的挑战。例如，由于在乐于助人和诚实之间的权衡失衡，经过RL调整的LLM可能会表现出过度保守。</li>\n<li><strong>GPT4使用合成幻觉数据来训练奖励模型并执行R</strong>L，从而将TruthfulQA的准确率从约30%提高到60%。</li>\n<li>设计一种专门用于减轻幻觉的特殊奖励函数:”Unhedged&#x2F;Hedged Correct&#x2F;Wrong”，指的是LLM用肯定或犹豫的语气提供正确或错误的答案。</li>\n</ol>\n<h3 id=\"4-4-生成推理阶段-设计解码策略\"><a href=\"#4-4-生成推理阶段-设计解码策略\" class=\"headerlink\" title=\"4.4 生成推理阶段-设计解码策略\"></a>4.4 生成推理阶段-设计解码策略</h3><p>在事实性方面，核采样(又称顶点采样)不如贪婪解码，可归因于top-p采样为提高多样性而引入的随机性，这可能会无意中导致幻觉，因为LLMs往往会编造信息以产生多样化的反应。</p>\n<ol>\n<li><strong>引入事实核采样的解码算法</strong>，旨在利用top-p和贪婪解码的优势，在多样性和事实性之间取得更有效的平衡。</li>\n<li>“<strong>验证链” (Chain-of-Verification, COVE )的解码框架</strong>。该框架基于这样的观察：独立的验证问题通常比长形式答案中提出的问题产生更准确的事实。COVE框架首先规划验证问题，然后回答这些问题，最终产生一个强化的、修正的回应。在多种任务上的实验结果表明，COVE能够有效缓解幻觉。</li>\n<li><strong>引入推理-时间干预(ITI)方法</strong>，以提高LLM的真实性。该方法基于这样一个假设，即LLMs拥有与真实性相关的潜在的、可解释的子结构。ITI方法包括两个步骤:<ol>\n<li>在LLM的每个注意头之上拟合一个二元分类器，以确定一组在回答事实性问题时具有卓越线性探测准确性的注意头;</li>\n<li>在推理过程中沿着这些与事实性相关的方向移动模型激活。</li>\n</ol>\n</li>\n<li><strong>检索增强设置</strong><ol>\n<li>LLMs在处理下游任务时有时无法充分关注检索到的知识，尤其是当检索到的知识与LLMs的参数知识相冲突时。为了解决这个问题，可以采用直接的上下文感知解码(<strong>CAD</strong>)策略。</li>\n<li><strong>CAD</strong>方法旨在迫使LLM更多地关注上下文信息，而不是过度依赖自身的参数知识来做出决策。实验结果表明，CAD能有效激发LLM利用检索知识的能力，从而减少下游任务中的事实幻觉</li>\n</ol>\n</li>\n<li><strong>修改模型结构</strong><ol>\n<li>多分支解码器和不确定性感知解码器。在构建LLM时采用双向自回归架构，从而实现从左到右和从右到左的语言建模，这种设计策略可以有效地利用双向自回归结构，有效地利用双向信息，有助于减少幻觉。</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"4-5-生成推理阶段-借助外部知识\"><a href=\"#4-5-生成推理阶段-借助外部知识\" class=\"headerlink\" title=\"4.5 生成推理阶段-借助外部知识\"></a>4.5 生成推理阶段-借助外部知识</h3><p>使用外部知识作为补充证据来帮助LLMs提供真实的回复，是最近兴起的一种解决方案。这种方法通常包括两个步骤:第一步是准确获取与用户指令相关的知识。一旦获得了有用的知识，第二步就需要利用这些知识来指导应答的生成。</p>\n<ol>\n<li><strong>知识获取</strong><ol>\n<li>外部知识库：大规模非结构化语料库、结构化数据库、维基百科等特定网站，甚至整个互联网</li>\n<li>外部工具：FacTool针对特定的下游任务，利用不同的工具帮助检测LLM中的幻觉，如用于基于知识的质量保证的搜索引擎API、用于代码生成的代码执行器和用于科学文献审查的谷歌学术API。</li>\n<li>CRITIC让LLM与多个工具交互，并自动修改其响应，这已被证明能有效提高真实性。</li>\n</ol>\n</li>\n<li><strong>知识利用</strong><ol>\n<li><strong>生成时补充</strong><ol>\n<li>利用检索到的知识或工具反馈的直接方法是，在提示LLMs之前直接将它们与用户查询串联起来，这种方法既有效又易于实施，这种知识也被称为内文知识。</li>\n</ol>\n</li>\n<li><strong>事后纠正</strong><ol>\n<li>即在后处理阶段构建一个辅助固定器来纠正幻觉。固定器可以是另一个LLM，也可以是一个特定的小模型。这种固定器首先与外部知识源互动，收集足够的知识，然后纠正幻觉。</li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"4-6-生成推理阶段-利用不确定性\"><a href=\"#4-6-生成推理阶段-利用不确定性\" class=\"headerlink\" title=\"4.6 生成推理阶段-利用不确定性\"></a>4.6 生成推理阶段-利用不确定性</h3><p>不确定性是推理过程中保护和减少幻觉的重要指标。通常，它指的是模型结果的置信度。不确定性可以帮助用户确定何时信任LLM。只要能准确描述LLM响应的不确定性，用户就能过滤或纠正LLM的高不确定性声明，因为这类声明更容易是捏造的。</p>\n<p><img src=\"/image%205.png\"></p>\n<ol>\n<li><p><strong>基于Logit的估计</strong></p>\n<ol>\n<li>这是一种基于对数的方法，它需要获取模型的logit，通常通过计算token级概率或熵来确定不确定性。</li>\n</ol>\n</li>\n<li><p><strong>其次是基于口头估计</strong></p>\n<ol>\n<li>直接要求LLM表达其不确定度，例如使用以下提示:”请回答并提供您的置信度分数(从0到100)”。这种方法之所以有效，是因为当地语言学家的语言表达能力和服从指令的能力很强。也可以使用思维链提示来加强这种方法。</li>\n</ol>\n</li>\n<li><p><strong>基于一致性估计</strong></p>\n<ol>\n<li>这种方法基于这样一个假设:当LLMs犹豫不决并对事实产生幻觉时，他们很可能会对同一问题做出逻辑上不一致的回答。</li>\n<li>使用BERTScore、基于QA的指标和n-gram指标进行计算，并将这些方法结合起来能产生最佳结果。直接利用额外的LLM来判断两个LLM反应在相同语境下是否存在逻辑矛盾，可以采用另一种LLM来修正两个反应中这种自相矛盾的幻觉。</li>\n</ol>\n</li>\n<li><p><strong>多agent互动</strong></p>\n<p><img src=\"/image%206.png\"></p>\n</li>\n</ol>\n<p>多个LLM(也称为代理)独立提出建议，并就各自的回应进行协作辩论，以达成单一共识。</p>\n<h3 id=\"4-3-个人总结\"><a href=\"#4-3-个人总结\" class=\"headerlink\" title=\"4.3 个人总结\"></a>4.3 个人总结</h3><p>缓解大模型幻觉业务中相较可行的措施：</p>\n<ol>\n<li><strong>构建高质量数据集：</strong><ol>\n<li>利用模型筛选出可能导致幻觉的数据并筛除。</li>\n<li>预训练或SFT阶段给忠实度更高的数据加权，或者只使用可靠来源的数据（例如经过人工筛查的数据，医典、教科书等）。</li>\n</ol>\n</li>\n<li><strong>优化模型结构：</strong><ol>\n<li>检索增强被证明可以显著减少幻觉问题。</li>\n<li>在解码时减少模型的随机性（例如，业务中已经采用的降低temperture的方法）。因为divisersity和faithfulness是一个trade-off的关系，减少diversity或randomness可以变相提高faithfulness&#x2F;factuality。</li>\n<li>设计更能充分编码以利用source information的方法。例如GNN网络，融入一些人类偏置。</li>\n</ol>\n</li>\n<li><strong>优化训练方式：</strong><ol>\n<li><strong>多任务学习</strong>，通过设计合适的额外任务，可以达到减轻幻觉的效果。</li>\n<li><strong>后处理</strong>，设计一个小模型，专门处理幻觉的问题。</li>\n<li><strong>提前规划骨架</strong>，再生成（sketch to content）。</li>\n<li>**<del>可控文本生成</del>**，将幻觉视为一种额外的属性，利用可控文本生成技术进行控制。</li>\n<li><strong>~~强化学习</strong>，~~现有工作将减轻模型的幻觉作为reward函数，从而减轻幻觉现象。</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><ol>\n<li><a href=\"https://arxiv.org/abs/2202.03629\">Survey of Hallucination in Natural Language Generation</a></li>\n<li><a href=\"https://github.com/HillZhang1999/llm-hallucination-survey\">Reading list of hallucination in LLMs</a></li>\n<li>Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499 (2021).</li>\n<li>Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. arXiv preprint arXiv:2206.04624 (2022).</li>\n<li>Chaojun Wang and Rico Sennrich. 2020. On exposure bias, hallucination and domain shift in neural machine translation. In Proceedings of the 2020 Annual Conference of the Association for Computational Linguistics. 3544–3552.</li>\n<li>Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP’21). 7052–7063.</li>\n<li>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’21). 4812–4829</li>\n<li>Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the ACL. 5055–5070.</li>\n<li>Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2021. Evaluating groundedness in dialogue systems: The BEGIN benchmark. In Findings of the Association for Computational Linguistics. Association for Computational Linguistics, 1–12.</li>\n<li>Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 3592–3603.</li>\n<li>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of Wikipedia: Knowledge-powered conversational agents. In Proceedings of the International Conference on Learning Representations.</li>\n<li>Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. 2021. GO FIGURE: A meta evaluation of factuality in summarization. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, 478–487.</li>\n<li>Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. Q2: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 7856–7870</li>\n<li>Vectara：让你的LLM应用告别幻觉！：<a href=\"https://zhuanlan.zhihu.com/p/626544154\">https://zhuanlan.zhihu.com/p/626544154</a></li>\n<li>Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. arXiv preprint arXiv:2206.04624 (2022).</li>\n<li>Peng, Baolin, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang et al. “Check your facts and try again: Improving large language models with external knowledge and automated feedback.”arXiv preprint arXiv:2302.12813(2023).</li>\n<li>annah Rashkin, David Reitter, Gaurav Singh Tomar, and Dipanjan Das. 2021. Increasing faithfulness in knowledgegrounded dialogue with controllable features. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. 704–718.</li>\n<li>Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang, Xiang Gao, Chris Quirk, Rik Koncel-Kedziorski, et al. 2021. A controllable model of grounded response generation. In Proceedings of the AAAI Conference on Artificial Intelligence. 14085–14093.</li>\n<li>Ratish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-text generation with content selection and planning. In Proceedings of the AAAI Conference on Artificial Intelligence</li>\n<li>Yangming Li, Kaisheng Yao, Libo Qin, Wanxiang Che, Xiaolong Li, and Ting Liu. 2020. Slot-consistent NLG for task-oriented dialogue systems with iterative rectification network. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 97–106.</li>\n<li>Mohsen Mesgar, Edwin Simpson, and Iryna Gurevych. 2021. Improving factual consistency between a response and persona facts. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics. 549–562.</li>\n<li>Sihao Chen, Fan Zhang, Kazoo Sone, and Dan Roth. 2021. Improving faithfulness in abstractive summarization with contrast candidate generation and selection. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’21). 5935–5941.</li>\n<li>大模型的幻觉问题调研: LLM Hallucination Survey: [<a href=\"https://zhuanlan.zhihu.com/p/642648601]\">https://zhuanlan.zhihu.com/p/642648601]</a>(</li>\n</ol>\n","excerpt":"","more":"<h1 id=\"大模型幻觉\"><a href=\"#大模型幻觉\" class=\"headerlink\" title=\"大模型幻觉\"></a>大模型幻觉</h1><h2 id=\"一、什么是-大模型幻觉问题？\"><a href=\"#一、什么是-大模型幻觉问题？\" class=\"headerlink\" title=\"一、什么是 大模型幻觉问题？\"></a>一、什么是 大模型幻觉问题？</h2><h3 id=\"1-1-大模型幻觉问题定义\"><a href=\"#1-1-大模型幻觉问题定义\" class=\"headerlink\" title=\"1.1 大模型幻觉问题定义\"></a>1.1 大模型幻觉问题定义</h3><ul>\n<li>定义：当模型生成的<strong>文本不遵循原文（Faithfulness）或者不符合事实（Factualness）</strong>，我们就可以认为模型出现了幻觉的问题。</li>\n</ul>\n<h3 id=\"1-2-何为-Faithfulness-and-Factualness？\"><a href=\"#1-2-何为-Faithfulness-and-Factualness？\" class=\"headerlink\" title=\"1.2 何为 Faithfulness and Factualness？\"></a>1.2 何为 Faithfulness and Factualness？</h3><ul>\n<li>Faithfulness：是否遵循input content；</li>\n<li>Factualness：是否符合世界知识；</li>\n</ul>\n<p><img src=\"/2024/09/01/hello-world/image.png\"></p>\n<p>（参考：Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models，<a href=\"https://arxiv.org/abs/2309.01219\">https://arxiv.org/abs/2309.01219</a>）</p>\n<p><img src=\"/2024/09/01/hello-world/image1.png\"></p>\n<ol>\n<li><strong>输入相冲突的幻觉。</strong>输入相冲突的幻觉，即LLM生成的内容与用户提供的源输入相背离。当LLM生成的内容偏离用户输入时，就会产生这类幻觉。</li>\n<li><strong>语境冲突性幻觉。</strong>LLM生成的内容与之前生成的信息本身相冲突。LLMs在生成冗长或多轮回答时可能会表现出自我矛盾。这种类型的幻觉产生于LLMs在整个对话过程中失去对上下文的跟踪或无法保持一致性时，这可能是由于他们在保持长期记忆或识别相关上下文方面的局限性造成的。</li>\n<li><strong>与事实相冲突的幻觉。</strong>与事实相冲突的幻觉指的是LLM生成的内容不忠实于既定的世界知识。事实冲突型幻觉。当LLM生成的信息或文本与已有的世界知识相矛盾时，就会出现这种类型的幻觉。如图2所示，事实冲突幻觉的来源可能多种多样，并在LLM生命周期的不同阶段出现。</li>\n</ol>\n<h3 id=\"除幻觉外的常见问题\"><a href=\"#除幻觉外的常见问题\" class=\"headerlink\" title=\"除幻觉外的常见问题:\"></a>除幻觉外的常见问题:</h3><p><img src=\"/2024/09/01/hello-world/image2.png\"></p>\n<ol>\n<li><strong>歧义性：</strong>答案可能并不一定是不正确的，但是对于用户问题却没有给出一个有用的答案。</li>\n<li><strong>不完整性：</strong>不完整性问题发生在生成的响应不完整或碎片化的情况下，如上图例子中只给出了换轮胎4个步骤中的2步。</li>\n<li><strong>偏见：</strong>LLMs中的偏见是指生成文本中不公平或偏见态度的表现，这些偏见可能来源于训练数据。如上图例子中将教师描述为女性是一种偏见。</li>\n<li><strong>信息不足：</strong>指LLMs逃避回答某些问题的倾向，即使其有能力这样做。例如，由于奖励模型的不完善，RLHF可能会导致LLMs的过度优化，从而可能导致模型输出信息不足。</li>\n</ol>\n<h3 id=\"1-3-传统任务中的模型幻觉-vs-LLMs-中模型幻觉\"><a href=\"#1-3-传统任务中的模型幻觉-vs-LLMs-中模型幻觉\" class=\"headerlink\" title=\"1.3 传统任务中的模型幻觉 vs LLMs 中模型幻觉\"></a>1.3 传统任务中的模型幻觉 vs LLMs 中模型幻觉</h3><ol>\n<li><strong>在传统任务里，幻觉大都是指的是Faithfulness</strong>：<ol>\n<li><strong>Intrinsic Hallucination（信息冲突）</strong>: LMs在生成回复时，与输入信息产生了冲突，例如摘要问题里，abstract和document的信息不一致；</li>\n<li><strong>Extrinsic Hallucination（无中生有）</strong>: LMs在生成回复时，输出一些并没有体现在输入中的额外信息，比如邮箱地址、电话号码、住址，并且难以验证其真假。（PS: 按照此定义，Extrinsic Hallucination有可能是真的信息，只是需要外部信息源进行认证）</li>\n</ol>\n</li>\n<li><strong>而面向LLMs，我们通常考虑的幻觉则是Factualness</strong>：<ol>\n<li>因为我们应用LLM的形式是open-domain Chat，而不是局限于特定任务，所以数据源可以看做任意的世界知识。LLMs如果生成了不在input source里的额外信息，但是符合事实的，这种情况也可能是对我们有帮助的。</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"二、大模型幻觉从何而来？\"><a href=\"#二、大模型幻觉从何而来？\" class=\"headerlink\" title=\"二、大模型幻觉从何而来？\"></a>二、大模型幻觉从何而来？</h2><h3 id=\"2-1-从-数据角度-进行分析\"><a href=\"#2-1-从-数据角度-进行分析\" class=\"headerlink\" title=\"2.1 从 数据角度 进行分析\"></a>2.1 从 数据角度 进行分析</h3><p>在 数据构建过程中，由于以下问题，导致 模型幻觉 的 发生：</p>\n<ol>\n<li>训练数据可信度问题。由于 大模型 的 训练数据 都是 通过 众包&#x2F;爬虫检索 方式 收集得到的，这种数据构建方式的优点是量比较大，但是缺点是 包含 大量虚假信息。这种虚假信息 直接导致的问题就是使 模型出现错误认知；</li>\n<li>重复数据问题。过多的重复信息也可能导致模型的知识记忆出现bias，从而导致幻觉；</li>\n<li>LLM偏向于肯定测试的样本，即人类生成的语料中也存在幻觉(可反映为过时的、双重的或捏造的表达)，LLMs很容易复制甚至放大这种幻觉行为。</li>\n</ol>\n<blockquote>\n<p>引用至 [3] Deduplicating training data makes language models better</p>\n</blockquote>\n<h3 id=\"2-2-从-模型角度-进行分析\"><a href=\"#2-2-从-模型角度-进行分析\" class=\"headerlink\" title=\"2.2 从 模型角度 进行分析\"></a>2.2 从 模型角度 进行分析</h3><p>不止是 数据角度问题，大模型幻觉问题 出现的原因 还 表现在 模型角度。</p>\n<ul>\n<li><strong>模型结构</strong>：如果是较弱的backbone（比如RNN）可能导致比较严重的幻觉问题，但在LLMs时代应该不太可能存在这一问题；</li>\n<li><strong>解码算法</strong>：<ul>\n<li>研究表明，<strong>如果使用不确定性较高的采样算法（e.g.，top-p）会诱导LMs出现更严重的幻觉问题</strong>。甚至可以故意在解码算法中加入一些随机性，进一步让LMs胡编乱造（可以用该方法生成一些negative samples）</li>\n<li>局部最优化(标记预测)并不一定能确保全局最优化(序列预测)，早期的局部预测可能会将LLMs带入难以形成正确反应的境地。</li>\n</ul>\n</li>\n</ul>\n<p>引用至 [4] Factuality enhanced language models for open-ended text generation</p>\n<ul>\n<li><strong>暴露偏差</strong>：<strong>训练和测试阶段不匹配的exposure bias问题可能导致LLMs出现幻觉，特别是生成long-form response的时候</strong>。</li>\n</ul>\n<blockquote>\n<p>引用至 [5] On exposure bias, hallucination and domain shift in neural machine translation</p>\n</blockquote>\n<ul>\n<li><strong>参数知识</strong>：<strong>LMs在预训练阶段记忆的错误的知识，将会严重导致幻觉问题</strong>。</li>\n</ul>\n<blockquote>\n<p>引用至 [6] Entity-based knowledge conflicts in question answering</p>\n</blockquote>\n<h2 id=\"三、如何-评估-大模型幻觉问题？\"><a href=\"#三、如何-评估-大模型幻觉问题？\" class=\"headerlink\" title=\"三、如何 评估 大模型幻觉问题？\"></a>三、如何 评估 大模型幻觉问题？</h2><p><strong>现有的传统幻觉评估指标和人类结果的相关性往往较低</strong>，同时大都是task-specific的 [7]。</p>\n<h3 id=\"3-1-Reference-based\"><a href=\"#3-1-Reference-based\" class=\"headerlink\" title=\"3.1 Reference-based\"></a>3.1 Reference-based</h3><p>Reference-based的指标有两类：</p>\n<ol>\n<li><strong>基于Source Information和Target Reference</strong>：利用一些统计学指标，比如<strong>ROUGE、BLEU来评估输出结果和Source&#x2F;Target信息的重叠度</strong>;</li>\n<li><strong>基于Source Information</strong>：由于NLG任务里，Target输出往往是多种多样的，因此许多工作<strong>只基于Source信息进行幻觉的评估</strong>。比如Knowledge F1。</li>\n</ol>\n<p>基于Reference的评价指标，<strong>基本上只能评价Faithfulness，而无法评价Factualness，因此通常不适用于LLMs</strong>。</p>\n<h3 id=\"3-2-Reference-Free\"><a href=\"#3-2-Reference-Free\" class=\"headerlink\" title=\"3.2 Reference-Free\"></a>3.2 Reference-Free</h3><h3 id=\"3-2-1-基于IE\"><a href=\"#3-2-1-基于IE\" class=\"headerlink\" title=\"3.2.1 基于IE\"></a>3.2.1 基于IE</h3><ul>\n<li>介绍：<strong>将知识限定于可以用三元组形式表示的关系和事件</strong>，基于额外的IE模型进行抽取，接着使用额外模型进行验证；</li>\n<li>缺点：<ul>\n<li>可能存在IE模型的错误传播问题；</li>\n<li>知识被限定在三元组形式。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-2-2-基于QA\"><a href=\"#3-2-2-基于QA\" class=\"headerlink\" title=\"3.2.2 基于QA\"></a>3.2.2 基于QA</h3><ul>\n<li>介绍：</li>\n</ul>\n<ol>\n<li>第一步先<strong>基于LM生成的回复</strong>，使用一个QG(question generation)模型生成一系列QA pairs；</li>\n<li>第二步<strong>给定Source Information，让QA模型对上一步生成的Question进行回复</strong>；</li>\n<li>第三步则是<strong>通过对比第一步的answers和第二步的answers，计算匹配指标，衡量模型的幻觉问题</strong>；</li>\n</ol>\n<ul>\n<li>缺点：可能存在IE模型的错误传播问题；难以评估Factualness，因为上述第二步里面，Source Information不可能包含全部的世界知识，因此对于一些问题难以生成可靠的回复。</li>\n</ul>\n<blockquote>\n<p>引用至 [8] FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization</p>\n</blockquote>\n<h3 id=\"3-2-3-基于NLI\"><a href=\"#3-2-3-基于NLI\" class=\"headerlink\" title=\"3.2.3 基于NLI\"></a>3.2.3 基于NLI</h3><ul>\n<li>介绍：基于NLI的方法<strong>通过NLI模型评估是否Source Information可以蕴含Generated Text，从而评估是否出现了幻觉现象</strong>。<ul>\n<li>缺点：<ul>\n<li>Off-the-shelf NLI模型用于核查事实效果不是很好</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>引用至 [9] Evaluating groundedness in dialogue systems: The BEGIN benchmark.</p>\n</blockquote>\n<ul>\n<li>无法评估需要世界知识的幻觉问题：<ul>\n<li><strong>仅能依赖于Source进行核查</strong>；</li>\n<li>都是sentence-level的，<strong>无法支撑更细粒度的幻觉检查；</strong></li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>引用至 [10] Evaluating factuality in generation with dependency-level entailment.</p>\n</blockquote>\n<ul>\n<li>幻觉问题和蕴含问题实际并不等价：</li>\n</ul>\n<ol>\n<li>例子：Putin is president. -&gt; Putin is U.S. president (可以蕴含，但是是幻觉)</li>\n</ol>\n<h3 id=\"3-2-4-基于Factualness-Classification-Metric\"><a href=\"#3-2-4-基于Factualness-Classification-Metric\" class=\"headerlink\" title=\"3.2.4 基于Factualness Classification Metric\"></a>3.2.4 基于Factualness Classification Metric</h3><ul>\n<li>介绍：标注&#x2F;构造一批和幻觉&#x2F;事实有关的数据，训练检测模型，利用该模型评估新生成文本的幻觉&#x2F;事实问题。</li>\n</ul>\n<blockquote>\n<p>引用至 [11] Knowledge-powered conversational agents</p>\n</blockquote>\n<h3 id=\"3-2-5-人工评估\"><a href=\"#3-2-5-人工评估\" class=\"headerlink\" title=\"3.2.5 人工评估\"></a>3.2.5 人工评估</h3><ul>\n<li>介绍：目前为止最靠谱的，此外还可以依靠LLM打分（比如利用GPT4，但是GPT4也存在着严重的幻觉问题，即使经过retrival-augment，检索回来的信息也有可能是错误的）</li>\n</ul>\n<h2 id=\"四、如何-缓解-大模型幻觉问题？\"><a href=\"#四、如何-缓解-大模型幻觉问题？\" class=\"headerlink\" title=\"四、如何 缓解 大模型幻觉问题？\"></a>四、如何 缓解 大模型幻觉问题？</h2><h3 id=\"4-1预训练阶段\"><a href=\"#4-1预训练阶段\" class=\"headerlink\" title=\"4.1预训练阶段\"></a>4.1预训练阶段</h3><p>LLM的知识大多是在预训练阶段获得的。在预训练语料库中存在诸如错误信息之类的噪声数据可能会破坏LLMs的参数知识，而这是导致误差的一个重要因素。有时，还可以将语言模型获得的事实知识追溯到其训练数据。<strong>因此，减少幻觉的直观方法可以是人工或自动整理预训练语料库，尽可能减少无法验证或不可靠的数据。</strong></p>\n<ol>\n<li><strong>人工消除噪声训练数据</strong><ol>\n<li>专注于”数据到文本”(data-to-text)任务，并邀请人工根据给定的知识库手动编写干净准确的回复。</li>\n<li>在现有的表格到文本数据集中对文本进行人工提炼这一过程也大大减少了事实幻觉。</li>\n<li>在构建表对文训练数据时，指导注释者修改维基百科中已验证的句子，而不是直接创建新句子。</li>\n</ol>\n</li>\n<li><strong>自动选择可靠数据或过滤掉噪声数据</strong><ol>\n<li><strong>GPT-3</strong>的预训练数据就是通过与一系列高质量参考数据的相似性进行清理。</li>\n<li><strong>Falcon</strong>通过启发式规则从网络中仔细提取高质量数据，并证明了经过适当分级的相关语料库可以产生强大的LLM。为了减少幻觉，目前的LLM通常会从可靠的文本来源收集预训练数据。</li>\n<li><strong>Llama2</strong>在构建预训练语料库时，从维基百科等高度事实性的来源中向上抽取数据</li>\n<li>在事实性文档的句子前加上主题前缀，使每个句子在预训练时都能作为一个独立的事实，将文档名称作为主题前缀，并观察到这种方法提高了LLM在<strong>TruthfulQA</strong>上的性能<strong>。</strong></li>\n</ol>\n</li>\n</ol>\n<h3 id=\"4-2-SFT阶段\"><a href=\"#4-2-SFT阶段\" class=\"headerlink\" title=\"4.2 SFT阶段\"></a>4.2 SFT阶段</h3><p><img src=\"/image%203.png\"></p>\n<p>当前的LLMs一般都会经历一个被称为”监督微调”(SFT)的过程，以从预训练中获取所需的知识，并学习如何与用户互动。SFT通常包括首先注释或收集海量任务指令跟踪数据，然后使用极大似然法(MLE)在这些数据上对预先训练的基础LLM进行微调。</p>\n<p><strong>目的</strong></p>\n<ul>\n<li>学习如何与用户进行有效交互。</li>\n<li>从预训练中获得的知识中抽取信息。</li>\n</ul>\n<ol>\n<li><strong>加工整理数据</strong><ol>\n<li>与预训练类似，一种可能的方法是要减少SFT阶段的幻觉，可以对训练数据进行整理，SFT数据量相对较小。</li>\n<li>手动和自动整理都是可行的选择。在未经编辑的数据上进行微调的LLM相比，在这些经过编辑的指令数据上进行微调的LLM表现出更高的真实性和事实性水平。</li>\n</ol>\n</li>\n<li><strong>加入拒答</strong><ol>\n<li>在推理过程中，如果被要求回答与未学知识相关的问题，他们很可能会自信地产生幻觉。</li>\n<li>采用以诚实为导向的SFT，即在SFT数据中引入一些诚实样本。诚实样本指的是承认自己无能的回答，如”对不起，我不知道”。学会拒绝回答特定问题，从而帮助减少幻觉。</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"4-3-RLHF期间的缓解\"><a href=\"#4-3-RLHF期间的缓解\" class=\"headerlink\" title=\"4.3 RLHF期间的缓解\"></a>4.3 RLHF期间的缓解</h3><p><img src=\"/image%204.png\"></p>\n<ol>\n<li>利用人类反馈不仅能缩小机器生成的内容与人类偏好之间的差距，还能帮助LLM符合预期的标准或目标。目前常用的一个标准是”3H”，即有益、诚实和无害。这里的”诚实”只是指在LLM反应中尽量减少幻觉。</li>\n<li>强化学习可以引导LLM探索其知识边界，使他们能够拒绝回答超出其能力范围的问题，而不是编造不真实的回答。但这种方法也带来了独特的挑战。例如，由于在乐于助人和诚实之间的权衡失衡，经过RL调整的LLM可能会表现出过度保守。</li>\n<li><strong>GPT4使用合成幻觉数据来训练奖励模型并执行R</strong>L，从而将TruthfulQA的准确率从约30%提高到60%。</li>\n<li>设计一种专门用于减轻幻觉的特殊奖励函数:”Unhedged&#x2F;Hedged Correct&#x2F;Wrong”，指的是LLM用肯定或犹豫的语气提供正确或错误的答案。</li>\n</ol>\n<h3 id=\"4-4-生成推理阶段-设计解码策略\"><a href=\"#4-4-生成推理阶段-设计解码策略\" class=\"headerlink\" title=\"4.4 生成推理阶段-设计解码策略\"></a>4.4 生成推理阶段-设计解码策略</h3><p>在事实性方面，核采样(又称顶点采样)不如贪婪解码，可归因于top-p采样为提高多样性而引入的随机性，这可能会无意中导致幻觉，因为LLMs往往会编造信息以产生多样化的反应。</p>\n<ol>\n<li><strong>引入事实核采样的解码算法</strong>，旨在利用top-p和贪婪解码的优势，在多样性和事实性之间取得更有效的平衡。</li>\n<li>“<strong>验证链” (Chain-of-Verification, COVE )的解码框架</strong>。该框架基于这样的观察：独立的验证问题通常比长形式答案中提出的问题产生更准确的事实。COVE框架首先规划验证问题，然后回答这些问题，最终产生一个强化的、修正的回应。在多种任务上的实验结果表明，COVE能够有效缓解幻觉。</li>\n<li><strong>引入推理-时间干预(ITI)方法</strong>，以提高LLM的真实性。该方法基于这样一个假设，即LLMs拥有与真实性相关的潜在的、可解释的子结构。ITI方法包括两个步骤:<ol>\n<li>在LLM的每个注意头之上拟合一个二元分类器，以确定一组在回答事实性问题时具有卓越线性探测准确性的注意头;</li>\n<li>在推理过程中沿着这些与事实性相关的方向移动模型激活。</li>\n</ol>\n</li>\n<li><strong>检索增强设置</strong><ol>\n<li>LLMs在处理下游任务时有时无法充分关注检索到的知识，尤其是当检索到的知识与LLMs的参数知识相冲突时。为了解决这个问题，可以采用直接的上下文感知解码(<strong>CAD</strong>)策略。</li>\n<li><strong>CAD</strong>方法旨在迫使LLM更多地关注上下文信息，而不是过度依赖自身的参数知识来做出决策。实验结果表明，CAD能有效激发LLM利用检索知识的能力，从而减少下游任务中的事实幻觉</li>\n</ol>\n</li>\n<li><strong>修改模型结构</strong><ol>\n<li>多分支解码器和不确定性感知解码器。在构建LLM时采用双向自回归架构，从而实现从左到右和从右到左的语言建模，这种设计策略可以有效地利用双向自回归结构，有效地利用双向信息，有助于减少幻觉。</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"4-5-生成推理阶段-借助外部知识\"><a href=\"#4-5-生成推理阶段-借助外部知识\" class=\"headerlink\" title=\"4.5 生成推理阶段-借助外部知识\"></a>4.5 生成推理阶段-借助外部知识</h3><p>使用外部知识作为补充证据来帮助LLMs提供真实的回复，是最近兴起的一种解决方案。这种方法通常包括两个步骤:第一步是准确获取与用户指令相关的知识。一旦获得了有用的知识，第二步就需要利用这些知识来指导应答的生成。</p>\n<ol>\n<li><strong>知识获取</strong><ol>\n<li>外部知识库：大规模非结构化语料库、结构化数据库、维基百科等特定网站，甚至整个互联网</li>\n<li>外部工具：FacTool针对特定的下游任务，利用不同的工具帮助检测LLM中的幻觉，如用于基于知识的质量保证的搜索引擎API、用于代码生成的代码执行器和用于科学文献审查的谷歌学术API。</li>\n<li>CRITIC让LLM与多个工具交互，并自动修改其响应，这已被证明能有效提高真实性。</li>\n</ol>\n</li>\n<li><strong>知识利用</strong><ol>\n<li><strong>生成时补充</strong><ol>\n<li>利用检索到的知识或工具反馈的直接方法是，在提示LLMs之前直接将它们与用户查询串联起来，这种方法既有效又易于实施，这种知识也被称为内文知识。</li>\n</ol>\n</li>\n<li><strong>事后纠正</strong><ol>\n<li>即在后处理阶段构建一个辅助固定器来纠正幻觉。固定器可以是另一个LLM，也可以是一个特定的小模型。这种固定器首先与外部知识源互动，收集足够的知识，然后纠正幻觉。</li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"4-6-生成推理阶段-利用不确定性\"><a href=\"#4-6-生成推理阶段-利用不确定性\" class=\"headerlink\" title=\"4.6 生成推理阶段-利用不确定性\"></a>4.6 生成推理阶段-利用不确定性</h3><p>不确定性是推理过程中保护和减少幻觉的重要指标。通常，它指的是模型结果的置信度。不确定性可以帮助用户确定何时信任LLM。只要能准确描述LLM响应的不确定性，用户就能过滤或纠正LLM的高不确定性声明，因为这类声明更容易是捏造的。</p>\n<p><img src=\"/image%205.png\"></p>\n<ol>\n<li><p><strong>基于Logit的估计</strong></p>\n<ol>\n<li>这是一种基于对数的方法，它需要获取模型的logit，通常通过计算token级概率或熵来确定不确定性。</li>\n</ol>\n</li>\n<li><p><strong>其次是基于口头估计</strong></p>\n<ol>\n<li>直接要求LLM表达其不确定度，例如使用以下提示:”请回答并提供您的置信度分数(从0到100)”。这种方法之所以有效，是因为当地语言学家的语言表达能力和服从指令的能力很强。也可以使用思维链提示来加强这种方法。</li>\n</ol>\n</li>\n<li><p><strong>基于一致性估计</strong></p>\n<ol>\n<li>这种方法基于这样一个假设:当LLMs犹豫不决并对事实产生幻觉时，他们很可能会对同一问题做出逻辑上不一致的回答。</li>\n<li>使用BERTScore、基于QA的指标和n-gram指标进行计算，并将这些方法结合起来能产生最佳结果。直接利用额外的LLM来判断两个LLM反应在相同语境下是否存在逻辑矛盾，可以采用另一种LLM来修正两个反应中这种自相矛盾的幻觉。</li>\n</ol>\n</li>\n<li><p><strong>多agent互动</strong></p>\n<p><img src=\"/image%206.png\"></p>\n</li>\n</ol>\n<p>多个LLM(也称为代理)独立提出建议，并就各自的回应进行协作辩论，以达成单一共识。</p>\n<h3 id=\"4-3-个人总结\"><a href=\"#4-3-个人总结\" class=\"headerlink\" title=\"4.3 个人总结\"></a>4.3 个人总结</h3><p>缓解大模型幻觉业务中相较可行的措施：</p>\n<ol>\n<li><strong>构建高质量数据集：</strong><ol>\n<li>利用模型筛选出可能导致幻觉的数据并筛除。</li>\n<li>预训练或SFT阶段给忠实度更高的数据加权，或者只使用可靠来源的数据（例如经过人工筛查的数据，医典、教科书等）。</li>\n</ol>\n</li>\n<li><strong>优化模型结构：</strong><ol>\n<li>检索增强被证明可以显著减少幻觉问题。</li>\n<li>在解码时减少模型的随机性（例如，业务中已经采用的降低temperture的方法）。因为divisersity和faithfulness是一个trade-off的关系，减少diversity或randomness可以变相提高faithfulness&#x2F;factuality。</li>\n<li>设计更能充分编码以利用source information的方法。例如GNN网络，融入一些人类偏置。</li>\n</ol>\n</li>\n<li><strong>优化训练方式：</strong><ol>\n<li><strong>多任务学习</strong>，通过设计合适的额外任务，可以达到减轻幻觉的效果。</li>\n<li><strong>后处理</strong>，设计一个小模型，专门处理幻觉的问题。</li>\n<li><strong>提前规划骨架</strong>，再生成（sketch to content）。</li>\n<li>**<del>可控文本生成</del>**，将幻觉视为一种额外的属性，利用可控文本生成技术进行控制。</li>\n<li><strong>~~强化学习</strong>，~~现有工作将减轻模型的幻觉作为reward函数，从而减轻幻觉现象。</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><ol>\n<li><a href=\"https://arxiv.org/abs/2202.03629\">Survey of Hallucination in Natural Language Generation</a></li>\n<li><a href=\"https://github.com/HillZhang1999/llm-hallucination-survey\">Reading list of hallucination in LLMs</a></li>\n<li>Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499 (2021).</li>\n<li>Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. arXiv preprint arXiv:2206.04624 (2022).</li>\n<li>Chaojun Wang and Rico Sennrich. 2020. On exposure bias, hallucination and domain shift in neural machine translation. In Proceedings of the 2020 Annual Conference of the Association for Computational Linguistics. 3544–3552.</li>\n<li>Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP’21). 7052–7063.</li>\n<li>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’21). 4812–4829</li>\n<li>Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the ACL. 5055–5070.</li>\n<li>Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2021. Evaluating groundedness in dialogue systems: The BEGIN benchmark. In Findings of the Association for Computational Linguistics. Association for Computational Linguistics, 1–12.</li>\n<li>Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 3592–3603.</li>\n<li>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of Wikipedia: Knowledge-powered conversational agents. In Proceedings of the International Conference on Learning Representations.</li>\n<li>Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. 2021. GO FIGURE: A meta evaluation of factuality in summarization. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, 478–487.</li>\n<li>Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. Q2: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 7856–7870</li>\n<li>Vectara：让你的LLM应用告别幻觉！：<a href=\"https://zhuanlan.zhihu.com/p/626544154\">https://zhuanlan.zhihu.com/p/626544154</a></li>\n<li>Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. arXiv preprint arXiv:2206.04624 (2022).</li>\n<li>Peng, Baolin, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang et al. “Check your facts and try again: Improving large language models with external knowledge and automated feedback.”arXiv preprint arXiv:2302.12813(2023).</li>\n<li>annah Rashkin, David Reitter, Gaurav Singh Tomar, and Dipanjan Das. 2021. Increasing faithfulness in knowledgegrounded dialogue with controllable features. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. 704–718.</li>\n<li>Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang, Xiang Gao, Chris Quirk, Rik Koncel-Kedziorski, et al. 2021. A controllable model of grounded response generation. In Proceedings of the AAAI Conference on Artificial Intelligence. 14085–14093.</li>\n<li>Ratish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-text generation with content selection and planning. In Proceedings of the AAAI Conference on Artificial Intelligence</li>\n<li>Yangming Li, Kaisheng Yao, Libo Qin, Wanxiang Che, Xiaolong Li, and Ting Liu. 2020. Slot-consistent NLG for task-oriented dialogue systems with iterative rectification network. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 97–106.</li>\n<li>Mohsen Mesgar, Edwin Simpson, and Iryna Gurevych. 2021. Improving factual consistency between a response and persona facts. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics. 549–562.</li>\n<li>Sihao Chen, Fan Zhang, Kazoo Sone, and Dan Roth. 2021. Improving faithfulness in abstractive summarization with contrast candidate generation and selection. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’21). 5935–5941.</li>\n<li>大模型的幻觉问题调研: LLM Hallucination Survey: [<a href=\"https://zhuanlan.zhihu.com/p/642648601]\">https://zhuanlan.zhihu.com/p/642648601]</a>(</li>\n</ol>\n"},{"title":"test_pict","date":"2024-09-01T14:27:28.000Z","_content":"![一张示例图片](image.png)\n","source":"_posts/test-pict.md","raw":"---\ntitle: test_pict\ndate: 2024-09-01 22:27:28\n---\n![一张示例图片](image.png)\n","slug":"test-pict","published":1,"updated":"2024-09-01T14:51:55.835Z","_id":"cm0jo1xf80000spioha2mggjc","comments":1,"layout":"post","photos":[],"content":"<p><img src=\"/2024/09/01/hello-world/age.png\" alt=\"一张示例图片\"></p>\n","excerpt":"","more":"<p><img src=\"/2024/09/01/hello-world/age.png\" alt=\"一张示例图片\"></p>\n"}],"PostAsset":[{"_id":"source/_posts/image.png","slug":"","post":"cm0jjfwt500014iio16bic1qh","modified":0,"renderable":0},{"_id":"source/_posts/LlmIllusion/image 3.png","slug":"image 3.png","post":"cm0jjfwt500014iio16bic1qh","modified":0,"renderable":0},{"_id":"source/_posts/LlmIllusion/image 4.png","slug":"image 4.png","post":"cm0jjfwt500014iio16bic1qh","modified":0,"renderable":0},{"_id":"source/_posts/LlmIllusion/image 5.png","slug":"image 5.png","post":"cm0jjfwt500014iio16bic1qh","modified":0,"renderable":0},{"_id":"source/_posts/LlmIllusion/image 6.png","slug":"image 6.png","post":"cm0jjfwt500014iio16bic1qh","modified":0,"renderable":0},{"_id":"source/_posts/LlmIllusion/image.png","slug":"image.png","post":"cm0jjfwt500014iio16bic1qh","modified":0,"renderable":0},{"_id":"source/_posts/test-pict/image.png","slug":"age.png","post":"cm0jjfwt500014iio16bic1qh","modified":0,"renderable":0},{"_id":"source/_posts/LlmIllusion/image1.png","slug":"image1.png","post":"cm0jjfwt500014iio16bic1qh","modified":0,"renderable":0},{"_id":"source/_posts/LlmIllusion/image2.png","slug":"image2.png","post":"cm0jjfwt500014iio16bic1qh","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cm0jk8pjy00028uiofoxk54dm","category_id":"cm0jk8pk200038uio0tsxe9i7","_id":"cm0jk8pk500068uio8rxe1xvx"}],"PostTag":[{"post_id":"cm0jk8pjy00028uiofoxk54dm","tag_id":"cm0jk8pk400048uiob6o58axv","_id":"cm0jk8pk500058uiodkwd3d9m"}],"Tag":[{"name":"大模型","_id":"cm0jk8pk400048uiob6o58axv"}]}}